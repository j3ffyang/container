<!--
<img src=../imgs/vantiq_logo2.png width=200px align="right">
<br>
<br>
-->

# Vantiq Deployment on Kubernetes Cluster

<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=4 orderedList=false} -->
<!-- code_chunk_output -->

- [Objective](#objective)
- [Network Overview](#network-overview)
- [Install Vantiq Product](#install-vantiq-product)
    - [Pre-requisite](#pre-requisite)
    - [VANTIQ Quick start](#vantiq-quick-start)
    - [configureClient](#configureclient)
    - [configureVantiqSystem](#configurevantiqsystem)
    - [Install Product](#install-product)
    - [Generate self-signed SSL Key](#generate-self-signed-ssl-key)
    - [Create Block Disks for Persistent Volume (PV)](#create-block-disks-for-persistent-volume-pv)
    - [Deploy PostgreSQL by Helm](#deploy-postgresql-by-helm)
    - [Prepare an SMTP Service, example](#prepare-an-smtp-service-example)
    - [deployNginx](#deploynginx)
    - [deployShared](#deployshared)
    - [Download ```defaultbackend``` image on all other node](#download-defaultbackend-image-on-all-other-node)
    - [deployVantiq](#deployvantiq)
    - [Check the status](#check-the-status)
- [Undeploy, clean the installed environment and redeploy](#undeploy-clean-the-installed-environment-and-redeploy)
    - [undeploy](#undeploy)
    - [clean](#clean)
    - [Delete persistent volume (PV) prior to re-deploy](#delete-persistent-volume-pv-prior-to-re-deploy)
    - [re-deploy](#re-deploy)
- [Appendix](#appendix)
    - [Catpure token after install](#catpure-token-after-install)
    - [Tiny persistent volume tutorial](#tiny-persistent-volume-tutorial)
    - [Setup front-end load-balancer](#setup-front-end-load-balancer)
    - [Setup role-based access control for Helm](#setup-role-based-access-control-for-helm)
    - [Disks and PVs](#disks-and-pvs)
    - [Exception: ```Error: error when upgrading: current Tiller version is newer, use --force-upgrade to downgrade```](#exception-error-error-when-upgrading-current-tiller-version-is-newer-use-force-upgrade-to-downgrade)
    - [```./vantiqSystem/build/deploy/nginx/telegraf-nginx-config.yaml``` does not exist](#vantiqsystembuilddeploynginxtelegraf-nginx-configyaml-does-not-exist)
    - [Connection reset by peer when using "stable" chart](#connection-reset-by-peer-when-using-stable-chart)
    - [Error of ```ImagePullBackOff``` after finishing ```deployVantiq```](#error-of-imagepullbackoff-after-finishing-deployvantiq)
    - [Force delete a hanging pod](#force-delete-a-hanging-pod)
    - [Check log of ```keycloak-init``` container and its secret](#check-log-of-keycloak-init-container-and-its-secret)
    - [Update KeyCloak's Credential](#update-keycloaks-credential)
    - [Manage SSL](#manage-ssl)
    - [Rescale](#rescale)

<!-- /code_chunk_output -->


## Objective
- Install Vantiq product on a standard Kubernetes cluster.
- Trying to document every step-by-step during deployment, including workaround and tricks

## Network Overview

<center><img src="../imgs/20190503_cp_network_arch.png"></center>

## Install Vantiq Product

#### Pre-requisite

You need a Github ID, __granted access__ to Vantiq private repo at Github

- Install OpenJAVA

```bash
sudo apt install openjdk-8-jre-headless

ubuntu@vantiq01:~$ java -version
openjdk version "1.8.0_191"
OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.18.04.1-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
```

- Git version
```bash
ubuntu@vantiq01:~$ git version
git version 2.17.1
```

- Docker version and controllable by a non-root user
```bash
ubuntu@vantiq01:~$ docker version
Client:
 Version:           18.09.3
 API version:       1.39
 Go version:        go1.10.8
 Git commit:        774a1f4
 Built:             Thu Feb 28 06:53:11 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          18.09.3
  API version:      1.39 (minimum version 1.12)
  Go version:       go1.10.8
  Git commit:       774a1f4
  Built:            Thu Feb 28 05:59:55 2019
  OS/Arch:          linux/amd64
  Experimental:     false
```

#### VANTIQ Quick start

Attention:

1. Check `license.key` front-door URL which is related to clusterName `-Pcluster` and namespace to be created in K8s
2. The SSL certificate used by VANTIQ and Nginx both must be the same as the front-door URL

The default path = `~/k8sdeploy_tools` or specified otherwise

```sh
git clone https://github.com/Vantiq/k8sdeploy_tools.git
```

Edit `~/.gradle/gradle.properties`	to set values of `gitUsername` & `gitPassword`. If `~/.gradle` doesn't exist, manually create one or running `./gradlew configureVantiqSystem` would create one but with errors.

```sh
./gradlew configureVantiqSystem # targetCluster & vantiqSystem dirs are created

~/k8sdeploy_tools/targetCluster$ git checkout -b eda  # the branch must be the same sub-domain of front-door URL
Switched to a new branch 'eda'

# Doesn't need this when using `helm3`
./gradlew -Pcluster=eda configureClient

cp ~/.kube/config ~/k8sdeploy_tools/targetCluster/kubeconfig

./gradlew -Pcluster=eda clusterInfo
./gradlew -Pcluster=eda setupCluster
```

Edit `~/targetCluster/deploy.yaml` and `~/targetCluster/secrets.yaml`
Copy or move `cert.pem  key.pem  license.key  public.pem` into `~/targetCluster/sensitive`

Attention again:
The SSL certificate used by VANTIQ and Nginx both must be the same as the front-door URL

```sh
./gradlew -Pcluster=eda generateSecrets
```

---

#### configureClient

- Download Code from Repo

```bash
git clone https://github.com/vantiq/k8sdeploy_tools.git #private repo
```

- Check References

> Reference - check parameters setting before starting > https://github.com/Vantiq/k8sdeploy_setup

Once you are ready, perform the following steps:

- Change directory to the directory under which you want to deploy the tools.  We recommend that this be your home directory or some sub-directory thereof.  If you already have a directory in which you put other Git repositories, then that is a good candidate as well.
- Run the command ```git clone https://github.com/Vantiq/k8sdeploy_tools.git```.  This will create a directory called ```k8sdeploy_tools``` in your current directory.  Depending on whether this is the first time you have used Git you may or may not be prompted for your credentials.
- Change directory to the newly create ```k8sdeploy_tools``` directory.  Unless otherwise specified, all commands are run from this directory.
- Run the command ```./gradlew  -Pcluster=<CLUSTER_NAME> configureClient```.  This command needs to download quite a bit of software, so depending on the speed of your network connection it may take some time to complete.  When it completes you should see something like this (there may be more text about welcome to Gradle, but these should be the last few lines of output):

```bash
  Starting a Gradle Daemon (subsequent builds will be faster)
  $HELM_HOME has been configured at /root/.helm.
  Not installing Tiller due to 'client-only' flag having been set
  Happy Helming!
  "vantiq" has been added to your repositories

  BUILD SUCCESSFUL in 20s
  3 actionable tasks: 3 executed
```

If you get an error, stop and contact Vantiq operations.

- Edit the file ```.gradle/gradle.properties```.  You should see the following contents:

```bash
  gitUsername=
  gitPassword=
```

Set the properties to be your GitHub username and password.  Remember that if you have two factor authentication (2FA) enabled for your account you will need to use a personal access key instead of your password.

#### configureVantiqSystem
- Run the command ```./gradlew configureVantiqSystem```.  If you see an error, stop and contact Vantiq operations.  This should create the directories ```vantiqSystem``` and ```targetCluster```.

You would need the access to https://github.com/Vantiq/k8sdeploy.git before the above command gets executed successfully

> Reference (this is an internal URL!) > Vantiq Kubernetes Deployment https://vantiq.sharepoint.com/:w:/r/_layouts/15/doc2.aspx?sourcedoc=%7Bd432b2c3-77bb-47f4-8ee2-0e3eeb3ccce3%7D&action=default&cid=6b0fbd49-a355-45cc-9fbe-bacb087dca88

```bash
git pull
./gradlew -- stop
./gradlew configureVantiqSystem
```

- Execute ```./gradlew configureVantiqSystem```

Create ```targetCluster``` and ```vantiqSystem```

Sample of ```./targetCluster/cluster.properties``` in customer's env at the moment the env is deployed.
```bash
# Property definitions for this cluster

# Select a provider for the cluster
provider=kubeadm
# Uncomment if the cluster comes with Helm already installed
#helm_installed=true
# Uncomment if you need to use a non-default version of the K8s clients
#kube_image_tag=2.11.0

# What version and configuration of the Vantiq system are we using
vantiq_system_release=1.1.3
deployment=development
vantiq.installation=eda

# Uncomment this if you intend to manage the definition of this cluster "locally" (aka on this machine only)
requireRemote=false
```

#### Install Product

> Reference > k8sdeploy_tools https://github.com/Vantiq/k8sdeploy_tools/blob/master/README.md

- Create a Git branch

```bash
cd k8sdeploy_tools/targetCluster
git checkout -b capheat
```

> Note: ```capheat``` is whatever ```clusterName``` you want, which is equal to ```Pcluster=```

- Create ```kubeconfig```

```bash
cp ~/.kube/config  ~/k8sdeploy_tools/targetCluster/kubeconfig
```

- Managing Cluster Definitions Locally

> Reference > Managing Cluster Definitions Locally https://github.com/Vantiq/k8sdeploy_tools/blob/master/docs/LocallyManagedDefinition.md

```bash
./gradlew -Pcluster=<CLUSTER_NAME> clusterInfo
./gradlew -Pcluster=<CLUSTER_NAME> setupCluster
```

- Install ```tiller```

```bash
./helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 --stable-repo-url http://mirror.azure.cn/kubernetes/charts/
```

```shell
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1
docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1 gcr.io/kubernetes-helm/tiller:v2.14.1
docker rmi  registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1
```

#### Generate self-signed SSL Key

- Create self-signed key

```bash
openssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 -keyout key.pem -out cert.pem
```

- Get serial number from a certificate

```bash
openssl x509 -in cert.pem -serial -noout
```

- Get a public key from a certificate

```bash
openssl x509 -inform pem -in cert.pem -pubkey -noout
```

- Inspect a cert

```bash
openssl x509 -in cert.pem -text -noout
```

```shell
cp *.pem ~/k8sdeploy_tools/targetCluster/deploy/certificates/	# key.pem cert.pem
cp *.pem ~/k8sdeploy_tools/targetCluster/deploy/vantiq/certificates/
```

#### Create Block Disks for Persistent Volume (PV)

On ```vantiq04```, where to install the following components

func | size in G | blk device | mount_point under /mnt/disks-by-id
--| -- | -- | --
postgresql for KeyCloak | 20 | vdb | disk0
influxDB | 160 | vdc |
grafana | 20 | vdd |
grafanaDB | 20 | vdg |

```bash
root@vantiq04:/mnt/disks-by-id# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0   20G  0 disk
└─vdb1 252:17   0   20G  0 part
vdc    252:32   0  160G  0 disk
└─vdc1 252:33   0  160G  0 part
vdd    252:48   0   20G  0 disk
└─vdd1 252:49   0   20G  0 part
vdg    252:96   0   20G  0 disk
└─vdg1 252:97   0   20G  0 part
root@vantiq04:/mnt/disks-by-id#
```

```bash
root@vantiq04:/mnt/disks-by-id# pwd
/mnt/disks-by-id
root@vantiq04:/mnt/disks-by-id# ls
disk0  disk1  disk2  disk3
```

Format then mount as
```bash
lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0   20G  0 disk
└─vdb1 252:17   0   20G  0 part /mnt/disks-by-id/disk0
vdc    252:32   0   20G  0 disk
vdd    252:48   0   20G  0 disk
vde    252:64   0  160G  0 disk
```

As long as one of the disks gets mounted, you should be able to view

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ kubectl -n default get all
...
NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/local-volume-provisioner   3         3         3       3            3           <none>          6d1h

NAME                                        READY   AGE
statefulset.apps/my-pg-release-postgresql   1/1     5m3s
```
and

```bash
ubuntu@vantiq2-test01:~$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
local-pv-7e0c864c   19Gi       RWO            Retain           Available           local-storage            14s
```

#### Deploy PostgreSQL by Helm

- Install Postgres
```bash
./helm install --name my-pg-release --set postgresqlDatabase=keycloak,postgresqlPassword=password,postgresqlUsername=keycloak,persistence.storageClass=local-storage stable/postgresql
```

- Helm delete and it also delete all associated resources, such as deployment

```bash
ubuntu@vantiq01:~/k8sdeploy_tools$ ./helm delete --purge my-pg-release
```

> Reference > https://github.com/helm/charts/tree/master/stable/postgresql

What we have about Postgres
```bash
ubuntu@vantiq01:~/k8sdeploy_tools$ kubectl get service
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP    20d
my-pg-release-postgresql   ClusterIP   10.98.172.253   <none>        5432/TCP   4m51s
```

#### Prepare an SMTP Service, example

- Customer's SMTP Configuration for reference. Specify in ```deploy.yaml```

protocol | server addr | port | ssl | starttls
-- | -- | -- | -- | --
POP | pop.qiye.163.com | 110 | 995
SMTP | smtp.qiye.163.com | 25 | 994 | 587
IMAP | imap.qiye.163.com | 143 | 993

#### deployNginx

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ ./gradlew -Pcluster=ch2 deployNginx
configmap/telegraf-nginx-config created
secret/vantiq-cert created
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "vantiq" chart repository
Update Complete. ⎈ Happy Helming!⎈
Release "ch2-lb" does not exist. Installing it now.
NAME:   ch2-lb
LAST DEPLOYED: Mon May 20 02:51:38 2019
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME                             AGE
ch2-lb-nginx-ingress-controller  0s

==> v1/ServiceAccount
ch2-lb-nginx-ingress  0s

==> v1beta1/ClusterRoleBinding
ch2-lb-nginx-ingress  0s

==> v1beta1/Role
ch2-lb-nginx-ingress  0s

==> v1/Service
ch2-lb-nginx-ingress-controller        0s
ch2-lb-nginx-ingress-controller-stats  0s
ch2-lb-nginx-ingress-default-backend   0s

==> v1beta1/ClusterRole
ch2-lb-nginx-ingress  0s

==> v1beta1/RoleBinding
ch2-lb-nginx-ingress  0s

==> v1beta1/DaemonSet
ch2-lb-nginx-ingress-controller  0s

==> v1beta1/Deployment
ch2-lb-nginx-ingress-default-backend  0s

==> v1/Pod(related)

NAME                                                   READY  STATUS             RESTARTS  AGE
ch2-lb-nginx-ingress-controller-92xxw                  0/2    ContainerCreating  0         0s
ch2-lb-nginx-ingress-controller-b25s4                  0/2    ContainerCreating  0         0s
ch2-lb-nginx-ingress-controller-xtk4n                  0/2    ContainerCreating  0         0s
ch2-lb-nginx-ingress-default-backend-84645757c8-q2rs8  0/1    ContainerCreating  0         0s


NOTES:
The nginx-ingress controller has been installed.
Get the application URL by running these commands:
  export HTTP_NODE_PORT=$(kubectl --namespace default get services -o jsonpath="{.spec.ports[0].nodePort}" ch2-lb-nginx-ingress-controller)
  export HTTPS_NODE_PORT=$(kubectl --namespace default get services -o jsonpath="{.spec.ports[1].nodePort}" ch2-lb-nginx-ingress-controller)
  export NODE_IP=$(kubectl --namespace default get nodes -o jsonpath="{.items[0].status.addresses[1].address}")

  echo "Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP."
  echo "Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS."

An example Ingress that makes use of the controller:

  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
    name: example
    namespace: foo
  spec:
    rules:
      - host: www.example.com
        http:
          paths:
            - backend:
                serviceName: exampleService
                servicePort: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
        - hosts:
            - www.example.com
          secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls


> Task :vantiqSystem:showIngressHost

Ingress LB Host:
BUILD SUCCESSFUL in 23s
11 actionable tasks: 11 executed
```

We can see Nginx ingress load-balance being created

```bash
service/ch2-lb-nginx-ingress-controller         NodePort    10.106.108.103   <none>        80:32488/TCP,443:30397/TCP   40s
```

Check the whole status

```bash
ubuntu@vantiq2-test01:~$ kubectl -n default get all
NAME                                                        READY   STATUS    RESTARTS   AGE
pod/busybox                                                 1/1     Running   47         47h
pod/ch2-lb-nginx-ingress-controller-92xxw                   2/2     Running   0          40s
pod/ch2-lb-nginx-ingress-controller-b25s4                   2/2     Running   0          40s
pod/ch2-lb-nginx-ingress-controller-xtk4n                   1/2     Running   0          40s
pod/ch2-lb-nginx-ingress-default-backend-84645757c8-q2rs8   1/1     Running   0          40s
pod/local-volume-provisioner-89z6d                          1/1     Running   0          8d
pod/local-volume-provisioner-jnlkz                          1/1     Running   0          8d
pod/local-volume-provisioner-m647f                          1/1     Running   0          8d
pod/my-pg-release-postgresql-0                              1/1     Running   0          2d13h

NAME                                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/ch2-lb-nginx-ingress-controller         NodePort    10.106.108.103   <none>        80:32488/TCP,443:30397/TCP   40s
service/ch2-lb-nginx-ingress-controller-stats   ClusterIP   10.111.3.102     <none>        18080/TCP                    40s
service/ch2-lb-nginx-ingress-default-backend    ClusterIP   10.104.56.68     <none>        80/TCP                       40s
service/kubernetes                              ClusterIP   10.96.0.1        <none>        443/TCP                      10d
service/my-pg-release-postgresql                ClusterIP   10.100.116.94    <none>        5432/TCP                     2d13h
service/my-pg-release-postgresql-headless       ClusterIP   None             <none>        5432/TCP                     2d13h

NAME                                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/ch2-lb-nginx-ingress-controller   3         3         2       3            2           <none>          40s
daemonset.apps/local-volume-provisioner          3         3         3       3            3           <none>          8d

NAME                                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ch2-lb-nginx-ingress-default-backend   1/1     1            1           40s

NAME                                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/ch2-lb-nginx-ingress-default-backend-84645757c8   1         1         1       40s

NAME                                        READY   AGE
statefulset.apps/my-pg-release-postgresql   1/1     2d13h
ubuntu@vantiq2-test01:~$
```

#### deployShared

```bash
ubuntu@vantiq01:~/k8sdeploy_tools$ ./gradlew -Pcluster=ch2 deployShared
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "vantiq" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈ Happy Helming!⎈
Release "grafanadb" does not exist. Installing it now.
NAME:   grafanadb
LAST DEPLOYED: Thu Apr 18 23:14:15 2019
NAMESPACE: shared
STATUS: DEPLOYED

RESOURCES:
==> v1beta1/Deployment
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
grafanadb-mysql  1        1        1           0          0s

==> v1/Pod(related)
NAME                              READY  STATUS   RESTARTS  AGE
grafanadb-mysql-59976d47bf-rchl8  0/1    Pending  0         0s

==> v1/Secret
NAME             TYPE    DATA  AGE
grafanadb-mysql  Opaque  2     0s

==> v1/ConfigMap
NAME                  DATA  AGE
grafanadb-mysql-test  1     0s

==> v1/PersistentVolumeClaim
NAME             STATUS   VOLUME         CAPACITY  ACCESS MODES  STORAGECLASS  AGE
grafanadb-mysql  Pending  local-storage  0s

==> v1/Service
NAME             TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE
grafanadb-mysql  ClusterIP  10.108.123.238  <none>       3306/TCP  0s


NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
grafanadb-mysql.shared.svc.cluster.local

To get your root password run:

    MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace shared grafanadb-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
    $ mysql -h grafanadb-mysql -p

To connect to your database directly from outside the K8s cluster:
    MYSQL_HOST=127.0.0.1
    MYSQL_PORT=3306

    # Execute the following command to route the connection:
    kubectl port-forward svc/grafanadb-mysql 3306

    mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}


Release "grafana" does not exist. Installing it now.
NAME:   grafana
LAST DEPLOYED: Thu Apr 18 23:14:18 2019
NAMESPACE: shared
STATUS: DEPLOYED

RESOURCES:
==> v1/Pod(related)
NAME                      READY  STATUS   RESTARTS  AGE
grafana-568bd6cb5b-72hxt  0/1    Pending  0         0s

==> v1/Secret
NAME     TYPE    DATA  AGE
grafana  Opaque  3     0s

==> v1beta1/Role
NAME     AGE
grafana  0s

==> v1beta1/RoleBinding
NAME     AGE
grafana  0s

==> v1/ServiceAccount
NAME     SECRETS  AGE
grafana  1        0s

==> v1/ClusterRole
NAME                 AGE
grafana-clusterrole  0s

==> v1/ClusterRoleBinding
NAME                        AGE
grafana-clusterrolebinding  0s

==> v1/Service
NAME     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE
grafana  ClusterIP  10.109.69.150  <none>       80/TCP   0s

==> v1beta2/Deployment
NAME     DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
grafana  1        1        1           0          0s

==> v1beta1/PodSecurityPolicy
NAME     PRIV   CAPS      SELINUX   RUNASUSER  FSGROUP   SUPGROUP  READONLYROOTFS  VOLUMES
grafana  false  RunAsAny  RunAsAny  RunAsAny   RunAsAny  false     configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim

==> v1/ConfigMap
NAME     DATA  AGE
grafana  1     0s

==> v1/PersistentVolumeClaim
NAME     STATUS   VOLUME         CAPACITY  ACCESS MODES  STORAGECLASS  AGE
grafana  Pending  local-storage  0s


NOTES:
1. Get your 'admin' user password by running:

   kubectl get secret --namespace shared grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.shared.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:

     export POD_NAME=$(kubectl get pods --namespace shared -l "app=grafana,release=grafana" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace shared port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin

Release "influxdb" does not exist. Installing it now.
NAME:   influxdb
LAST DEPLOYED: Thu Apr 18 23:14:25 2019
NAMESPACE: shared
STATUS: DEPLOYED

RESOURCES:
==> v1beta1/Deployment
NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
influxdb-influxdb  1        1        1           0          0s

==> v1/Pod(related)
NAME                                READY  STATUS   RESTARTS  AGE
influxdb-influxdb-8499b69bc5-xfk7r  0/1    Pending  0         0s

==> v1/ConfigMap
NAME               DATA  AGE
influxdb-influxdb  1     0s

==> v1/PersistentVolumeClaim
NAME               STATUS   VOLUME         CAPACITY  ACCESS MODES  STORAGECLASS  AGE
influxdb-influxdb  Pending  local-storage  0s

==> v1/Service
NAME               TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)            AGE
influxdb-influxdb  ClusterIP  10.105.37.117  <none>       8086/TCP,8088/TCP  0s


NOTES:
InfluxDB can be accessed via port 8086 on the following DNS name from within your cluster:

- http://influxdb-influxdb.shared:8086

You can easily connect to the remote instance with your local influx cli. To forward the API port to localhost:8086 run the following:

- kubectl port-forward --namespace shared $(kubectl get pods --namespace shared -l app=influxdb-influxdb -o jsonpath='{ .items[0].metadata.name }') 8086:8086

You can also connect to the influx cli from inside the container. To open a shell session in the InfluxDB pod run the following:

- kubectl exec -i -t --namespace shared $(kubectl get pods --namespace shared -l app=influxdb-influxdb -o jsonpath='{.items[0].metadata.name}') /bin/sh

To tail the logs for the InfluxDB pod run the following:

- kubectl logs -f --namespace shared $(kubectl get pods --namespace shared -l app=influxdb-influxdb -o jsonpath='{ .items[0].metadata.name }')

secret/keycloakdb created
Release "keycloak" does not exist. Installing it now.
NAME:   keycloak
LAST DEPLOYED: Thu Apr 18 23:14:33 2019
NAMESPACE: shared
STATUS: DEPLOYED

RESOURCES:
==> v1/Service
NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE
keycloak-headless  ClusterIP  None            <none>       80/TCP   0s
keycloak-http      ClusterIP  10.111.157.238  <none>       80/TCP   0s

==> v1beta1/StatefulSet
NAME      DESIRED  CURRENT  AGE
keycloak  1        1        0s

==> v1beta1/Ingress
NAME      HOSTS  ADDRESS  PORTS  AGE
keycloak  *      80       0s

==> v1/Pod(related)
NAME        READY  STATUS             RESTARTS  AGE
keycloak-0  0/1    ContainerCreating  0         0s

==> v1/Secret
NAME           TYPE    DATA  AGE
keycloak-http  Opaque  1     0s

==> v1/ConfigMap
NAME           DATA  AGE
keycloak       2     0s
keycloak-test  1     0s


NOTES:

Keycloak can be accessed:

* Within your cluster, at the following DNS name at port 80:

  keycloak-http.shared.svc.cluster.local

* From outside the cluster:
  - http://

Login with the following credentials:
Username: keycloak

To retrieve the initial user password run:
kubectl get secret --namespace shared keycloak-http -o jsonpath="{.data.password}" | base64 --decode; echo

Release "telegraf-ds" does not exist. Installing it now.
NAME:   telegraf-ds
LAST DEPLOYED: Thu Apr 18 23:14:35 2019
NAMESPACE: shared
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME                     DATA  AGE
telegraf-ds-telegraf-ds  1     1s

==> v1beta1/DaemonSet
NAME                     DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
telegraf-ds-telegraf-ds  0        0        0      0           0          <none>         1s

==> v1/Pod(related)
NAME                           READY  STATUS             RESTARTS  AGE
telegraf-ds-telegraf-ds-85dkt  0/1    ContainerCreating  0         1s
telegraf-ds-telegraf-ds-9k5xq  0/1    ContainerCreating  0         1s
telegraf-ds-telegraf-ds-s69t5  0/1    ContainerCreating  0         1s
telegraf-ds-telegraf-ds-stjdd  0/1    Pending            0         1s
telegraf-ds-telegraf-ds-wfr56  0/1    Pending            0         1s
telegraf-ds-telegraf-ds-xfpvg  0/1    Pending            0         1s


NOTES:
To open a shell session in the container running Telegraf run the following:

- kubectl exec -i -t --namespace shared $(kubectl get pods --namespace shared -l app=telegraf-ds-telegraf-ds -o jsonpath='{.items[0].metadata.name}') /bin/sh

To tail the logs for a Telegraf pod in the Daemonset run the following:

- kubectl logs -f --namespace shared $(kubectl get pods --namespace shared -l app=telegraf-ds-telegraf-ds -o jsonpath='{ .items[0].metadata.name }')

To list the running Telegraf instances run the following:

- kubectl get pods --namespace shared -l app=telegraf-ds-telegraf-ds -w

BUILD SUCCESSFUL in 32s
12 actionable tasks: 9 executed, 3 up-to-date
```

- Mount the block disks from small size. Check whether pods of ```grafana```, and ```grafana-mysql``` (20GB each) are launched successfully prior to mounting large disk for ```influxdb``` (150GB). The mount the disks for Mongo, which requires 500GB. This prevents ```grafana``` from attaching the big disk!!!

- Status check (example)

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools/vantiqSystem/deploy/shared$ kubectl -n shared get all
NAME                                     READY   STATUS    RESTARTS   AGE
pod/grafana-568bd6cb5b-r28hz             1/1     Running   2          3m50s
pod/grafanadb-mysql-59976d47bf-4q2zr     1/1     Running   0          3m54s
pod/influxdb-influxdb-8499b69bc5-xq25b   1/1     Running   0          3m47s
pod/keycloak-0                           1/1     Running   0          3m41s
pod/telegraf-ds-telegraf-ds-25jxm        1/1     Running   0          3m40s
pod/telegraf-ds-telegraf-ds-ggftn        1/1     Running   0          3m40s
pod/telegraf-ds-telegraf-ds-kz47g        1/1     Running   0          3m40s

NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/grafana             ClusterIP   10.101.219.99    <none>        80/TCP              3m50s
service/grafanadb-mysql     ClusterIP   10.107.255.112   <none>        3306/TCP            3m54s
service/influxdb-influxdb   ClusterIP   10.104.199.77    <none>        8086/TCP,8088/TCP   3m47s
service/keycloak-headless   ClusterIP   None             <none>        80/TCP              3m41s
service/keycloak-http       ClusterIP   10.105.157.47    <none>        80/TCP              3m41s

NAME                                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/telegraf-ds-telegraf-ds   3         3         3       3            3           <none>          3m40s

NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana             1/1     1            1           3m50s
deployment.apps/grafanadb-mysql     1/1     1            1           3m54s
deployment.apps/influxdb-influxdb   1/1     1            1           3m47s

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-568bd6cb5b             1         1         1       3m50s
replicaset.apps/grafanadb-mysql-59976d47bf     1         1         1       3m54s
replicaset.apps/influxdb-influxdb-8499b69bc5   1         1         1       3m47s

NAME                        READY   AGE
statefulset.apps/keycloak   1/1     3m41s
```

#### Download ```defaultbackend``` image on all other node

This workaround is optional if ```gcr.io``` unaccessible
```bash
imageName=defaultbackend:1.4
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
docker rmi  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
```

#### deployVantiq

```bash
ubuntu@vantiq01:~/k8sdeploy_tools$ ./gradlew -Pcluster=capheat deployvantiq
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "vantiq" chart repository
...Unable to get an update from the "stable" chart repository (https://kubernetes-charts.storage.googleapis.com):
        read tcp 172.17.0.2:53572->216.58.200.240:443: read: connection reset by peer
Update Complete. ⎈ Happy Helming!⎈
Release "vantiq-eda" does not exist. Installing it now.
NAME:   vantiq-eda
LAST DEPLOYED: Fri Apr 19 23:21:16 2019
NAMESPACE: eda
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME                     DATA  AGE
telegraf-mongodb-config  1     0s
loadmodel-config         3     0s
vantiq-config            8     0s

==> v1/RoleBinding
NAME        AGE
vantiq-eda  0s

==> v1/Service
NAME                TYPE          CLUSTER-IP      EXTERNAL-IP                             PORT(S)    AGE
vantiq-eda-mongodb  ClusterIP     10.106.98.1     <none>                                  27017/TCP  0s
keycloak-http       ExternalName  <none>          keycloak-http.shared.svc.cluster.local  80/TCP     0s
vantiq-headless     ClusterIP     None            <none>                                  8080/TCP   0s
vantiq              ClusterIP     10.102.143.167  <none>                                  8080/TCP   0s

==> v1beta1/CronJob
NAME         SCHEDULE  SUSPEND  ACTIVE  LAST SCHEDULE  AGE
mongobackup  @daily    False    0       <none>         0s

==> v1beta1/Ingress
NAME        HOSTS           ADDRESS  PORTS  AGE
vantiq-eda  eda.vantiq.com  80, 443  0s

==> v1/Pod(related)
NAME                                READY  STATUS    RESTARTS  AGE
vantiq-eda-mongodb-bc79ff5c8-nwpgl  0/2    Pending   0         0s
vantiq-eda-0                        0/1    Init:0/3  0         0s

==> v1/Secret
NAME            TYPE                            DATA  AGE
vantiq-cert     kubernetes.io/tls               2     0s
keycloak-init   Opaque                          4     0s
vantiq-mongodb  Opaque                          2     0s
regcred         kubernetes.io/dockerconfigjson  1     0s

==> v1/PersistentVolumeClaim
NAME                STATUS   VOLUME         CAPACITY  ACCESS MODES  STORAGECLASS  AGE
vantiq-eda-mongodb  Pending  local-storage  0s

==> v1/ServiceAccount
NAME        SECRETS  AGE
vantiq-eda  1        0s

==> v1/Role
NAME        AGE
vantiq-eda  0s

==> v1beta1/Deployment
NAME                DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
vantiq-eda-mongodb  1        1        1           0          0s

==> v1beta2/StatefulSet
NAME        DESIRED  CURRENT  AGE
vantiq-eda  1        1        0s


NOTES:
1. Get the application URL by running these commands:


> Task :vantiqSystem:deployVantiqServerForeda
NO VERSION given for chart ./vantiqSystem/build/deploy/vantiq/vantiq, installing latest stable version.

BUILD SUCCESSFUL in 4m 34s
7 actionable tasks: 4 executed, 3 up-to-date
```

- download vantiq-server manually on node5/6 with docker login

```bash
kubectl -n eda edit statefulsets.apps vantiq-eda
```

- Edit ```imagePullPolicy``` replace ```Always``` with ```IfNotPresent```

```bash
containers:
- args:
	- -Dio.vantiq.installation=eda
	image: vantiq/vantiq-server:1.24.12
	imagePullPolicy: IfNotPresent
```

- You can manually download the required images if image pull failed. Mostly the failure is caused by incorrect credentials in download session.

```shell
docker pull bitnami/mongodb:4.0.3-debian-9
docker pull vantiq/keycloak:4.2.1.Final
docker pull vantiq/vantiq-server:1.25.6
```
> vantiq/keycloak is the image from private repo


```bash
ubuntu@vantiq01:~/k8sdeploy_tools$ cat keycloak_token.txt
fDMXDOTxPTNV1U0SrWJxUhlfUhN1Zv5COFoeXAtb8OM=
ubuntu@vantiq01:~/k8sdeploy_tools$ kubectl get all -n eda
NAME                                     READY   STATUS    RESTARTS   AGE
pod/mongobackup-1555718400-4t9qs         0/1     Error     0          34m
pod/mongobackup-1555718400-6qplz         0/1     Error     0          27m
pod/mongobackup-1555718400-hlpph         0/1     Error     0          32m
pod/mongobackup-1555718400-l7dcx         0/1     Error     0          31m
pod/mongobackup-1555718400-pctcq         0/1     Error     0          33m
pod/mongobackup-1555718400-rkkrp         0/1     Error     0          32m
pod/mongobackup-1555718400-tw4j8         0/1     Error     0          30m
pod/vantiq-eda-0                         1/1     Running   0          4m22s
pod/vantiq-eda-mongodb-bc79ff5c8-p6g2s   2/2     Running   0          43m

NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP                              PORT(S)     AGE
service/keycloak-http        ExternalName   <none>           keycloak-http.shared.svc.cluster.local   80/TCP      43m
service/vantiq               ClusterIP      10.97.117.2      <none>                                   8080/TCP    43m
service/vantiq-eda-mongodb   ClusterIP      10.102.235.102   <none>                                   27017/TCP   43m
service/vantiq-headless      ClusterIP      None             <none>                                   8080/TCP    43m

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/vantiq-eda-mongodb   1/1     1            1           43m

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/vantiq-eda-mongodb-bc79ff5c8   1         1         1       43m

NAME                          READY   AGE
statefulset.apps/vantiq-eda   1/1     43m

NAME                               COMPLETIONS   DURATION   AGE
job.batch/mongobackup-1555718400   0/1           34m        34m

NAME                        SCHEDULE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/mongobackup   @daily     False     0        34m             43m

```

#### Check the status
- kube-system
```bash
ubuntu@vantiq2-test01:~$ kubectl -n kube-system get all
NAME                                         READY   STATUS    RESTARTS   AGE
pod/coredns-d5947d4b-9shhz                   1/1     Running   0          8d
pod/coredns-d5947d4b-psnhp                   1/1     Running   0          8d
pod/etcd-vantiq2-test01                      1/1     Running   0          8d
pod/kube-apiserver-vantiq2-test01            1/1     Running   0          8d
pod/kube-controller-manager-vantiq2-test01   1/1     Running   0          8d
pod/kube-flannel-ds-amd64-7m6z9              1/1     Running   1          8d
pod/kube-flannel-ds-amd64-85chm              1/1     Running   0          8d
pod/kube-flannel-ds-amd64-td9hp              1/1     Running   0          8d
pod/kube-flannel-ds-amd64-zqqs5              1/1     Running   0          8d
pod/kube-proxy-2c8l5                         1/1     Running   0          8d
pod/kube-proxy-ktpzf                         1/1     Running   0          8d
pod/kube-proxy-qlk4w                         1/1     Running   0          8d
pod/kube-proxy-xf82t                         1/1     Running   0          8d
pod/kube-scheduler-vantiq2-test01            1/1     Running   0          8d
pod/tiller-deploy-59b99695d8-pdmhs           1/1     Running   0          4d9h

NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns        ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   8d
service/tiller-deploy   ClusterIP   10.105.58.231   <none>        44134/TCP                6d13h

NAME                                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
daemonset.apps/kube-flannel-ds-amd64     4         4         4       4            4           beta.kubernetes.io/arch=amd64     8d
daemonset.apps/kube-proxy                4         4         4       4            4           <none>                            8d

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns         2/2     2            2           8d
deployment.apps/tiller-deploy   1/1     1            1           6d13h

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-d5947d4b           2         2         2       8d
replicaset.apps/tiller-deploy-59b99695d8   1         1         1       4d10h
replicaset.apps/tiller-deploy-5fbf5bcb96   0         0         0       6d13h
```

- default
```shell
ubuntu@vantiq2-test01:~$ kubectl -n default get all
NAME                                 READY   STATUS    RESTARTS   AGE
pod/local-volume-provisioner-89z6d   1/1     Running   0          6d13h
pod/local-volume-provisioner-jnlkz   1/1     Running   0          6d13h
pod/local-volume-provisioner-m647f   1/1     Running   0          6d13h
pod/my-pg-release-postgresql-0       1/1     Running   0          11h

NAME                                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes                          ClusterIP   10.96.0.1       <none>        443/TCP    8d
service/my-pg-release-postgresql            ClusterIP   10.100.116.94   <none>        5432/TCP   11h
service/my-pg-release-postgresql-headless   ClusterIP   None            <none>        5432/TCP   11h

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/local-volume-provisioner   3         3         3       3            3           <none>          6d13h

NAME                                        READY   AGE
statefulset.apps/my-pg-release-postgresql   1/1     11h
```

- shared
```bash
ubuntu@vantiq01:~$ kubectl get all -n shared
NAME                                     READY   STATUS    RESTARTS   AGE
pod/grafana-568bd6cb5b-72hxt             1/1     Running   0          39h
pod/grafanadb-mysql-59976d47bf-rchl8     1/1     Running   0          39h
pod/influxdb-influxdb-8499b69bc5-xfk7r   1/1     Running   0          39h
pod/keycloak-0                           1/1     Running   42         38h
pod/telegraf-ds-telegraf-ds-85dkt        1/1     Running   0          39h
pod/telegraf-ds-telegraf-ds-9k5xq        1/1     Running   0          39h
pod/telegraf-ds-telegraf-ds-s69t5        1/1     Running   0          39h
pod/telegraf-ds-telegraf-ds-stjdd        1/1     Running   0          39h
pod/telegraf-ds-telegraf-ds-wfr56        1/1     Running   0          39h
pod/telegraf-ds-telegraf-ds-xfpvg        1/1     Running   0          39h

NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/grafana             ClusterIP   10.109.69.150    <none>        80/TCP              39h
service/grafanadb-mysql     ClusterIP   10.108.123.238   <none>        3306/TCP            39h
service/influxdb-influxdb   ClusterIP   10.105.37.117    <none>        8086/TCP,8088/TCP   39h
service/keycloak-headless   ClusterIP   None             <none>        80/TCP              39h
service/keycloak-http       ClusterIP   10.111.157.238   <none>        80/TCP              39h

NAME                                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/telegraf-ds-telegraf-ds   6         6         6       6            6           <none>          39h

NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana             1/1     1            1           39h
deployment.apps/grafanadb-mysql     1/1     1            1           39h
deployment.apps/influxdb-influxdb   1/1     1            1           39h

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-568bd6cb5b             1         1         1       39h
replicaset.apps/grafanadb-mysql-59976d47bf     1         1         1       39h
replicaset.apps/influxdb-influxdb-8499b69bc5   1         1         1       39h

NAME                        READY   AGE
statefulset.apps/keycloak   1/1     39h
```

- vantiq-eda
```bash
ubuntu@vantiq2-test01:~$ kubectl -n eda-dev get all
NAME                                          READY   STATUS    RESTARTS   AGE
pod/vantiq-eda-dev-0                          1/1     Running   0          10h
pod/vantiq-eda-dev-mongodb-775cf9c77d-pldrj   2/2     Running   0          11h

NAME                             TYPE           CLUSTER-IP      EXTERNAL-IP                              PORT(S)     AGE
service/keycloak-http            ExternalName   <none>          keycloak-http.shared.svc.cluster.local   80/TCP      11h
service/vantiq                   ClusterIP      10.107.109.44   <none>                                   8080/TCP    11h
service/vantiq-eda-dev-mongodb   ClusterIP      10.107.104.49   <none>                                   27017/TCP   11h
service/vantiq-headless          ClusterIP      None            <none>                                   8080/TCP    11h

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/vantiq-eda-dev-mongodb   1/1     1            1           11h

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/vantiq-eda-dev-mongodb-775cf9c77d   1         1         1       11h

NAME                              READY   AGE
statefulset.apps/vantiq-eda-dev   1/1     11h

NAME                               COMPLETIONS   DURATION   AGE
job.batch/mongobackup-1558137600   0/1           46m        46m
```

Test the result

```bash
for i in {1..6}; do echo "hostname=1$i"; curl -v http://10.100.100.1$i:31978/healthz; done
```

## Undeploy, clean the installed environment and redeploy

#### undeploy
```shell
cd ~/k8sdeploy_tools
./gradlew -Pcluster=<CLUSTER_NAME> undeployvantiq
./gradlew -Pcluster=<CLUSTER_NAME> undeployShared
./gradlew -Pcluster=<CLUSTER_NAME> undeployNginx
```

#### clean
```bash
./gradlew -Pcluster=<CLUSTER_NAME> clean
```

#### Delete persistent volume (PV) prior to re-deploy

- ```rm -fr``` mounted persistent volumes very carefully

- umount mounted persistent volumes

- delete PVs

The deployed persistent volumes (PV) needs to be deleted with status = ```Available```
```bash
ubuntu@vantiq2-test01:~$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                                     STORAGECLASS    REASON   AGE
local-pv-476bb852   19Gi       RWO            Retain           Released   shared/grafana                            local-storage            2d13h
local-pv-7e0c864c   19Gi       RWO            Retain           Bound      default/data-my-pg-release-postgresql-0   local-storage            2d13h
local-pv-bf67d9f3   19Gi       RWO            Retain           Released   shared/grafanadb-mysql                    local-storage            2d13h
local-pv-d90de49d   156Gi      RWO            Retain           Released   shared/influxdb-influxdb                  local-storage            2d13h
ubuntu@vantiq2-test01:~$ kubectl delete pv local-pv-bf67d9f3
persistentvolume "local-pv-bf67d9f3" deleted
ubuntu@vantiq2-test01:~$ kubectl delete pv local-pv-476bb852
persistentvolume "local-pv-476bb852" deleted
ubuntu@vantiq2-test01:~$ kubectl delete pv local-pv-d90de49d
persistentvolume "local-pv-d90de49d" deleted
ubuntu@vantiq2-test01:~$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                     STORAGECLASS    REASON   AGE
local-pv-476bb852   19Gi       RWO            Retain           Available                                             local-storage            12s
local-pv-7e0c864c   19Gi       RWO            Retain           Bound       default/data-my-pg-release-postgresql-0   local-storage            2d13h
local-pv-bf67d9f3   19Gi       RWO            Retain           Available                                             local-storage            22s
local-pv-d90de49d   156Gi      RWO            Retain           Available                                             local-storage            2s
```

#### re-deploy
```bash
./gradlew -Pcluster=<CLUSTER_NAME> deployNginx
./gradlew -Pcluster=<CLUSTER_NAME> deployShared
./gradlew -Pcluster=<CLUSTER_NAME> deployVantiq
```

## Appendix

#### Catpure token after install

```bash
kubectl -n eda log pod/vantiq-eda-0
```

The token to setup KeyCloak can be found at the end of output, for example
```bash
01ZcoLZrTEi5LZlI-Vd82mbYyqAOA_CLYYJ2TnReQBs=
```

#### Tiny persistent volume tutorial

- Mount a 20G block device to ```/mnt/disks-by-id/disk0``` on any of node in K8S cluster

```bash
kubectl create ns test
kubectl apply -f testPvc.yaml
```

```bash
ubuntu@vantiq01:~/k8sdeploy_tools$ kubectl get pvc
NAME                STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS    AGE
my-pvc-test-pvc-0   Bound    local-pv-c66a60be   520Gi      RWO            local-storage   7m20s
```

The PVC will be bound to the 1st seen disk. So if you manually do this, you would have to mount and verify the claimed PVC one-by-one in manual steps

Don't forget to delete the test PV and PVC in the tutorial

```bash
kubectl delete pvc xxx
kubectl delete pv  xxx
```

#### Setup front-end load-balancer

Here is the example of env on OpenStack at customer

- The single IP to access https://eda.mydomain.com
```bash
  10.100.100.200
```

- IPs of Kubernetes cluster, that maps to front-end IP, excluding master_node

```bash
  10.100.100.11
  10.100.100.12
  10.100.100.13
  10.100.100.14
  10.100.100.15
  10.100.100.16
```

- ```NotePort```, from ```kubectl -n default get all```
```bash
  80:30820/TCP,443:30413/TCP
```

- Use ```TCP``` instead of ```HTTP``` in configuration of Load-Balancer front-end

- Provide self-signed key to OpenStack administrator who configures Load-Balancer

#### Setup role-based access control for Helm

> Reference > https://helm.sh/docs/using_helm/#tiller-and-role-based-access-control

- Create ```rbac-config.yaml```

```bash
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
```

- Create

```shell
kubectl apply -f rback-config.yaml
```

- Update Helm with ```service-account``` defined

```bash
./helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/ --force-upgrade
```

#### Disks and PVs

All disks for ```shared``` are located on ```vantiq-test04``` node

```bash
ubuntu@vantiq-test04:/mnt/disks-by-id$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0   20G  0 disk
└─vdb1 252:17   0   20G  0 part /var/lib/kubelet/pods/05b0c8cf-7129-11e9-a3a8-fa163edae6af/volumes/kubernetes.io~local-volume/local-p
vdc    252:32   0   20G  0 disk
└─vdc1 252:33   0   20G  0 part /var/lib/kubelet/pods/06dabac1-7129-11e9-a3a8-fa163edae6af/volumes/kubernetes.io~local-volume/local-p
vdd    252:48   0   20G  0 disk
└─vdd1 252:49   0   20G  0 part /var/lib/kubelet/pods/973d59f7-7126-11e9-a3a8-fa163edae6af/volumes/kubernetes.io~local-volume/local-p
vde    252:64   0  160G  0 disk
└─vde1 252:65   0  160G  0 part /var/lib/kubelet/pods/0961ecac-7129-11e9-a3a8-fa163edae6af/volumes/kubernetes.io~local-volume/local-p
```

```bash
ubuntu@vantiq-test01:~$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                     STORAGECLASS    REASON   AGE
local-pv-6e62dbcb   19Gi       RWO            Retain           Bound    shared/grafanadb-mysql                    local-storage            29h
local-pv-700d0498   19Gi       RWO            Retain           Bound    shared/grafana                            local-storage            29h
local-pv-ae57100f   19Gi       RWO            Retain           Bound    default/data-my-pg-release-postgresql-0   local-storage            4h15m
local-pv-d744676a   156Gi      RWO            Retain           Bound    shared/influxdb-influxdb                  local-storage            3h27m
ubuntu@vantiq-test01:~$ kubectl get pvc
NAME                              STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-my-pg-release-postgresql-0   Bound    local-pv-ae57100f   19Gi       RWO            local-storage   3h58m
```

#### Exception: ```Error: error when upgrading: current Tiller version is newer, use --force-upgrade to downgrade```

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ ./gradlew configureVantiqSystem
> Task :configureVantiqSystem FAILED

FAILURE: Build failed with an exception.

* Where:
Build file '/home/ubuntu/k8sdeploy_tools/build.gradle' line: 197

* What went wrong:
Execution failed for task ':configureVantiqSystem'.
> org.eclipse.jgit.api.errors.RefNotFoundException: Ref origin/new-master cannot be resolved

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 3s
1 actionable task: 1 executed
```

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ ./gradlew -Pcluster=ch2 setupCluster
$HELM_HOME has been configured at /root/.helm.
Error: error when upgrading: current Tiller version is newer, use --force-upgrade to downgrade
> Task :initHelmServer FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':initHelmServer'.
> COMMAND: vantiq_helm init --service-account tiller --upgrade

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 7s
5 actionable tasks: 2 executed, 3 up-to-date
```

Workaround:

```bash
./helm init --service-account tiller --force-upgrade --upgrade
```

#### ```./vantiqSystem/build/deploy/nginx/telegraf-nginx-config.yaml``` does not exist

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ ./gradlew -Pcluster=ch2 undeployNginx
error: the path "./vantiqSystem/build/deploy/nginx/telegraf-nginx-config.yaml" does not exist
> Task :vantiqSystem:deleteTelegrafConfig FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':vantiqSystem:deleteTelegrafConfig'.
> COMMAND: vantiq_kubectl delete -f ./vantiqSystem/build/deploy/nginx/telegraf-nginx-config.yaml --namespace default

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 7s
2 actionable tasks: 2 executed
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ ./gradlew -Pcluster=ch2 clean

BUILD SUCCESSFUL in 4s
2 actionable tasks: 2 executed
ubuntu@vantiq2-test01:~/k8sdeploy_tools$
```

Its actual path =
```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ find . -type f -name telegraf-nginx-config.yaml
./vantiqSystem/deploy/nginx/telegraf-nginx-config.yaml
```

#### Connection reset by peer when using "stable" chart

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ ./gradlew -Pcluster=ch2 deployShared
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "vantiq" chart repository
...Unable to get an update from the "stable" chart repository (https://kubernetes-charts.storage.googleapis.com):
        Get https://kubernetes-charts.storage.googleapis.com/index.yaml: read tcp 172.17.0.2:44004->172.217.24.48:443: read: connection reset by peer
Update Complete. ⎈ Happy Helming!⎈
```

Because ```googleapis.com``` is mostly blocked by firewall, the workaround of getting access to it

1. There is a bunch of IPs at front-end of ```googleapis.com```. Try ```nslookup kubernetes-charts.storage.googleapis.com``` to get some of IPs. For example

```bash
ubuntu@vantiq2-test02:~/ $ nslookup kubernetes-charts.storage.googleapis.com
Server:		172.16.51.100
Address:	172.16.51.100#53

Non-authoritative answer:
kubernetes-charts.storage.googleapis.com	canonical name = storage.l.googleusercontent.com.
Name:	storage.l.googleusercontent.com
Address: 172.217.31.240
Name:	storage.l.googleusercontent.com
Address: 2404:6800:4005:808::2010
```

Execute

```bash
ubuntu@vantiq2-test02:~/ $ telnet 172.217.31.240 443
Trying 172.217.31.240...
Connected to 172.217.31.240.
Escape character is '^]'.
^CConnection closed by foreign host.
```

This means the service at ```172.217.31.240``` is visible and accessible without being blocked by firewall

2. Then add the following into ```/etc/hosts```

```bash
172.217.31.240	kubernetes-charts.storage.googleapis.com
172.217.31.240	storage.l.googleusercontent.com
```

#### Error of ```ImagePullBackOff``` after finishing ```deployVantiq```

```bash
ubuntu@vantiq2-test01:~$ kubectl -n eda-dev get all
NAME                                          READY   STATUS                  RESTARTS   AGE
pod/vantiq-eda-dev-0                          0/1     Init:ImagePullBackOff   0          150m
```

Edit ```kubectl -n eda-dev edit statefulset.apps/vantiq-eda-dev```

```bash
image: docker.io/bitnami/mongodb:4.0.3-debian-9
imagePullPolicy: IfNotPresent
```

Then delete ```eda-dev``` pod to restart

#### Force delete a hanging pod

```shell
kubectl -n default delete pods mypod-ba97bc8ef-8rgaa --grace-period=0 --force
```

#### Check log of ```keycloak-init``` container and its secret

```bash
kubectl -n eda logs vantiq-eda-0 -c keycloak-init

kubectl -n eda get secrets keycloak-init -o yaml
kubectl -n shared get secrets keycloak-http -o yaml

kubectl -n shared get secret keycloak-http -o jsonpath="{.data.password}" | base64 --decode; echo

echo cGFzc3cwcmQ= | base64 --decode

## XXX = configureRealm.sh |
```

#### Update KeyCloak's Credential

```bash
kubectl -n shared exec -it keycloak-0 /bin/bash

/opt/jboss/keycloak/bin/add-user-keycloak.sh -r master -u keycloak -p passw0rd

/opt/jboss/keycloak/bin/add-user-keycloak.sh -u keycloak -p passw0rd

/opt/jboss/keycloak/bin/jboss-cli.sh --connect command="reload"
```

#### Manage SSL

- export ingress configuration

```sh
inesa@node07:~$ kubectl -n iot get ingress vantiq-iot -o yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: 100m
  creationTimestamp: "2020-02-20T19:39:21Z"
  generation: 1
  labels:
    app: vantiq
    chart: vantiq-0.2.0
    heritage: Tiller
    installation: iot
    release: vantiq-iot
  name: vantiq-iot
  namespace: iot
  resourceVersion: "33094266"
  selfLink: /apis/extensions/v1beta1/namespaces/iot/ingresses/vantiq-iot
  uid: 480deec4-994e-48ab-a911-2fd90c611d9c
spec:
  rules:
  - host: iot.interesting.net
    http:
      paths:
      - backend:
          serviceName: vantiq
          servicePort: 8080
        path: /
  tls:
  - hosts:
    - iot.interesting.net
    secretName: vantiq-cert
status:
  loadBalancer: {}
```

check `secretName: vantiq-cert` to match output of `kubectl -n iot get secrets vantiq-cert  -o yaml`

- list ingress configuration

```sh
inesa@node07:~$ kubectl -n iot get ingress vantiq-iot
NAME         HOSTS                   ADDRESS   PORTS     AGE
vantiq-iot   iot.interesting.net             80, 443   3d18h
```

- list lb-nginx-ingress pod configuration in yaml

```sh
inesa@node07:~$ kubectl -n default get pod/ins-lb-nginx-ingress-controller-wf4gf -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2020-02-20T01:07:30Z"
  generateName: ins-lb-nginx-ingress-controller-
  labels:
    app: nginx-ingress
    component: controller
    controller-revision-hash: 56b9fc44fc
    pod-template-generation: "1"
    release: ins-lb
  name: ins-lb-nginx-ingress-controller-wf4gf
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: DaemonSet
    name: ins-lb-nginx-ingress-controller
    uid: 531d9075-4b1e-48c9-a5fb-e07705288ac4
  resourceVersion: "32965428"
  selfLink: /api/v1/namespaces/default/pods/ins-lb-nginx-ingress-controller-wf4gf
  uid: ca6896dc-430b-43f6-961e-dc90a1fc966f
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: vantiq.com/workload-preference
            operator: DoesNotExist
        weight: 50
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: In
            values:
            - node11
  containers:
  - args:
    - /nginx-ingress-controller
    - --default-backend-service=default/ins-lb-nginx-ingress-default-backend
    - --publish-service=default/ins-lb-nginx-ingress-controller
    - --election-id=ingress-controller-leader
    - --ingress-class=nginx
    - --configmap=default/ins-lb-nginx-ingress-controller
    - --default-ssl-certificate=default/vantiq-cert
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.23.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 10254
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    name: nginx-ingress-controller
    ports:
    - containerPort: 80
      name: http
      protocol: TCP
    - containerPort: 443
      name: https
      protocol: TCP
    - containerPort: 18080
      name: stats
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 10254
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    resources: {}
    securityContext:
      capabilities:
        add:
        - NET_BIND_SERVICE
        drop:
        - ALL
      runAsUser: 33
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: ins-lb-nginx-ingress-token-44mdx
      readOnly: true
  - image: telegraf:1.8.1-alpine
    imagePullPolicy: IfNotPresent
    name: nginx-telegraf
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /etc/telegraf
      name: telegraf-config
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: ins-lb-nginx-ingress-token-44mdx
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: node11
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: ins-lb-nginx-ingress
  serviceAccountName: ins-lb-nginx-ingress
  terminationGracePeriodSeconds: 60
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/disk-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/pid-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/unschedulable
    operator: Exists
  volumes:
  - configMap:
      defaultMode: 420
      name: telegraf-nginx-config
    name: telegraf-config
  - name: ins-lb-nginx-ingress-token-44mdx
    secret:
      defaultMode: 420
      secretName: ins-lb-nginx-ingress-token-44mdx
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2020-02-20T01:07:30Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2020-02-20T01:07:52Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2020-02-20T01:07:52Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2020-02-20T01:07:30Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://1ba562c42940a9fee86c139576892973fbadd969f3cca2f5f46f8aef0b61b293
    image: quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller:0.23.0
    imageID: docker-pullable://quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller@sha256:e1292564ba5f1fd75005a4575778523d3309fb5d5d57f6597234c0b1567641f6
    lastState: {}
    name: nginx-ingress-controller
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: "2020-02-20T01:07:33Z"
  - containerID: docker://dbdd7ca3b1b7a8af18ca0d84a97af9598deb84a432312e61805d7668873cd293
    image: telegraf:1.8.1-alpine
    imageID: docker-pullable://telegraf@sha256:7f6873f5745fe9b4f610c5dcfaaa73d7cd75adb63bd770bb70c26a7e85c94a2a
    lastState: {}
    name: nginx-telegraf
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: "2020-02-20T01:07:33Z"
  hostIP: 192.168.100.38
  phase: Running
  podIP: 10.244.4.8
  qosClass: BestEffort
  startTime: "2020-02-20T01:07:30Z"
inesa@node07:~$
```

- list secret of SSL
```sh
kubectl -n iot get secrets vantiq-cert -o yaml
```
- create new keys then redeploy `vantiqNginx`

```sh
kc create secret tls foo --cert=cert.crt --key=key.pem
kubectl create secret tls foo --cert=cert.crt --key=key.pem
kubectl create secret tls foo --cert=main.pem --key=key.pem
```

#### Rescale
```sh
kubectl -n eda-dev scale --replicas={0,3} statefulset vantiq-eda-dev
```
