<!--
<img src=../imgs/vantiq_logo2.png width=200px align="right">
<br>
<br>
-->

# Deploy Kubernetes on a Private OpenStack Cloud
<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=4 orderedList=false} -->
<!-- code_chunk_output -->

- [Objective](#objective)
- [Existing Environment](#existing-environment)
- [Architecture Overview and Network Topology](#architecture-overview-and-network-topology)
    - [Environment Stack](#environment-stack)
    - [Network Topology](#network-topology)
- [Security Hardening](#security-hardening)
    - [Operating System on Virtual Machine](#operating-system-on-virtual-machine)
    - [Update Hostname](#update-hostname)
    - [Harden SSHd, then ```systemctl restart sshd.service```](#harden-sshd-then-systemctl-restart-sshdservice)
    - [Turn-on Firewall](#turn-on-firewall)
    - [Modify ```ufw``` on 1st host which has access to the world](#modify-ufw-on-1st-host-which-has-access-to-the-world)
    - [Create a non-root user and grant it ```sudo``` permission](#create-a-non-root-user-and-grant-it-sudo-permission)
- [Setup Network and Disk](#setup-network-and-disk)
    - [Update ```/etc/hosts``` on 1st host](#update-etchosts-on-1st-host)
    - [Setup all hostnames by using ```hostnamectl set-hostname```](#setup-all-hostnames-by-using-hostnamectl-set-hostname)
    - [Setup network to allow other hosts go internet](#setup-network-to-allow-other-hosts-go-internet)
    - [Block Device for Mount](#block-device-for-mount)
- [Install and Configure Docker](#install-and-configure-docker)
    - [Install](#install)
    - [(optional) Use China alternative image repo for performance purpose. Modify ```/etc/docker/daemon.json```](#optional-use-china-alternative-image-repo-for-performance-purpose-modify-etcdockerdaemonjson)
    - [~~Use ```systemd``` to control the Docker daemon~~](#~~use-systemd-to-control-the-docker-daemon~~)
    - [Grant non-root to control Docker](#grant-non-root-to-control-docker)
- [Install and Configure Kubernetes](#install-and-configure-kubernetes)
    - [Using China alternative repo for workaround of ```gcr.io``` unaccessible](#using-china-alternative-repo-for-workaround-of-gcrio-unaccessible)
    - [Init cluster by `kubeadm`](#init-cluster-by-kubeadm)
    - [Install ```flannel``` network](#install-flannel-network)
    - [Grant non-root user access to manage K8S](#grant-non-root-user-access-to-manage-k8s)
    - [```kubectl``` AutoComplete](#kubectl-autocomplete)
    - [Check the status by non-root user](#check-the-status-by-non-root-user)
    - [Join K8S cluster](#join-k8s-cluster)
    - [Setup helm](#setup-helm)
    - [```helm``` AutoComplete](#helm-autocomplete)
    - [Install Tiller](#install-tiller)
- [Operation](#operation)
    - [Update ```dnsConfig``` in ```coredns```](#update-dnsconfig-in-coredns)
    - [Connect Private Git Repo by using Personal Token](#connect-private-git-repo-by-using-personal-token)
    - [Save and Load Image in Docker](#save-and-load-image-in-docker)
    - [List all pods by Node](#list-all-pods-by-node)
    - [SSL Keys in K8S](#ssl-keys-in-k8s)
    - [Check expiration of SSL Cert](#check-expiration-of-ssl-cert)
    - [Rotate Kubernetes Certs/ Keys](#rotate-kubernetes-certs-keys)
    - [Kubernetes Backup & Restore](#kubernetes-backup-restore)
    - [Upgrade Kubernetes to a specific version](#upgrade-kubernetes-to-a-specific-version)
    - [List all running pods on all nodes](#list-all-running-pods-on-all-nodes)
    - [List all pods with `restart`](#list-all-pods-with-restart)
    - [Tear down a cluster](#tear-down-a-cluster)
    - [Audit `mount` and `umount`](#audit-mount-and-umount)
    - [Monitor Mongo disk in Prometheus](#monitor-mongo-disk-in-prometheus)
    - [Force delete a Pod](#force-delete-a-pod)
    - [MongoDB Backup and Restore with Docker and Kubernetes](#mongodb-backup-and-restore-with-docker-and-kubernetes)
    - [MongoDB Backup (without Docker and Kubernetes)](#mongodb-backup-without-docker-and-kubernetes)
    - [KeyCloak: update master realm](#keycloak-update-master-realm)
- [Troubleshooting](#troubleshooting)
    - [When joining the cluster](#when-joining-the-cluster)
    - [Exception Received during Deployment ```coredns``` STATUS= ```CrashLoopBackOff```](#exception-received-during-deployment-coredns-status-crashloopbackoff)
    - [Unable to delete an unused PersistentVolume (PV). Alway in ```terminating``` state](#unable-to-delete-an-unused-persistentvolume-pv-alway-in-terminating-state)

<!-- /code_chunk_output -->

## Objective
- Create a standard Kubernetes cluster for production on a standard OpenStack cloud environment without Magnum
- Specify how to create Persistent Volume and Persistent Volume Claim
- Harden the system while connecting to the internet
- Create iptables postrouting rule to enable all nodes access to internet through the 1st machine which has internet access

## Existing Environment

CPU 核 | Memory (G) 内存 | OS Disk (G) | IP | External Disk (G) 外挂磁盘
-- | -- | -- | -- | --
4 | 16 | 40 | 10.100.100.11 |
4 | 16 | 40 | 10.100.100.12 | 530 (MongoDB)
4 | 16 | 40 | 10.100.100.13 | 530 (MongoDB)
4 | 16 | 40 | 10.100.100.14 | 20 (Postgres for KeyCloak)/ 20 (Grafana)/ 20 (GrafanaDB)/ 165 (InfluxDB)
4 | 16 | 40 | 10.100.100.15 | 50 (Nexus)
4 | 16 | 40 | 10.100.100.16 |

> Notice: since the size of disks become a little smaller around 10% after being mounted as persistent volume (PV), so need to give a little more space for this tolerance

## Architecture Overview and Network Topology

#### Environment Stack

<center><img src="../imgs/20190420_vantiq_k8s_openstack.png"></center>

#### Network Topology
We're given total 6 virtual machines and one of 6 has access to internet

<img src="../imgs/20190327_cp_nw_top.png">

## Security Hardening

#### Operating System on Virtual Machine

```bash
root@vantiq01:~# cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION="Ubuntu 18.04.1 LTS"
```

#### Update Hostname
```bash
hostnamectl set-hostname vantiq01
```

#### Harden SSHd, then ```systemctl restart sshd.service```
```bash
PermitRootLogin prohibit-password

PubkeyAuthentication yes
PasswordAuthentication no
```

#### Turn-on Firewall
```sh
ufw allow ssh
ufw enable
```

#### Modify ```ufw``` on 1st host which has access to the world
```bash
sudo ufw status numbered
Status: active

     To                         Action      From
     --                         ------      ----
[ 1] OpenSSH                    ALLOW IN    Anywhere
[ 2] 22/tcp                     ALLOW IN    Anywhere
[ 3] 6443                       ALLOW IN    Anywhere
```

Generally, the iptables rule should allow the following access requirements

1. Request from ```10.100.100.0/24``` to ```10.100.100.11:6443```
2. Request from ```10.224.0.0/24``` to  ```10.100.100.11:6443``` by ```flannel```
3. The incoming from random Github IPs to ```10.100.100.11:6443```, associated to outgoing traffic when image download is initialized

Relative port required by Kubernetes > https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports


#### Create a non-root user and grant it ```sudo``` permission
```bash
adduser ubuntu
usermod -aG sudo ubuntu
```


## Setup Network and Disk

#### Update ```/etc/hosts``` on 1st host
```bash
10.100.100.11	vantiq01	vantiq01
10.100.100.12	vantiq02	vantiq02
10.100.100.13	vantiq03	vantiq03
10.100.100.14	vantiq04	vantiq04
10.100.100.15	vantiq05	vantiq05
10.100.100.16	vantiq06	vantiq06
```

#### Setup all hostnames by using ```hostnamectl set-hostname```

In Bash shell, run
```bash
for i in {2..6}; do ssh root@vantiq0$i \
  -i ~/.ssh/Vantiq-key.pem "hostnamectl set-hostname vantiq0$i"; done
```

```bash
for i in {2..6}; do ssh root@vantiq0$i -i ~/.ssh/Vantiq-key.pem "hostname"; done
```

#### Setup network to allow other hosts go internet
Since the 1st host is the only one allowed to access internet and others are off the world, here are the steps to route network requests from other hosts through the 1st one to internet

On the 1st host as ```gateway```, use ```POSTROUTING``` rule to ```MASQUERADE``` all traffic from ```10.100.100.0/24``` pass through ```10.100.100.11``` to reach outside. If ```ufw``` is on, need to define a rule for incoming traffic from ```10.100.100.0/24```

```bash
iptables -t nat -A POSTROUTING -s 10.100.100.0/24 -j MASQUERADE
ufw allow in on eth0 from 10.100.100.0/24
```

Update 1st host's ```/etc/default/ufw``` configuration to allow forward, then ```systemctl restart ufw.service```
```bash
DEFAULT_FORWARD_POLICY="ACCEPT"
```

On other hosts
```bash
ip r add default via 10.100.100.11 dev eth0
```

> Note: you might have to delete the pre-configured default gateway. Caution: be careful when you do this and you know what exactly you're doing!

#### Block Device for Mount

```bash
for i in {2..4}; do ssh root@vantiq0$i -i ~/.ssh/Vantiq-key.pem 'hostname; lsblk'; done
vantiq02
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0  530G  0 disk
└─vdb1 252:17   0  530G  0 part
vantiq03
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0  530G  0 disk
vantiq04
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0   20G  0 disk
vdc    252:32   0  160G  0 disk
vdg    252:96   0   20G  0 disk
```

## Install and Configure Docker

#### Install
> Reference > https://docs.docker.com/install/linux/docker-ce/ubuntu/

```bash {.line-numbers}
sudo apt install apt-transport-https ca-certificates curl software-properties-common

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

sudo add-apt-repository \
  "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

sudo apt update
sudo apt install docker-ce
```

#### (optional) Use China alternative image repo for performance purpose. Modify ```/etc/docker/daemon.json```
> Reference > https://www.docker-cn.com/registry-mirror

```json
{
  "registry-mirrors": ["https://registry.docker-cn.com"]
}
```

#### ~~Use ```systemd``` to control the Docker daemon~~
Overriding Defaults for the Docker Daemon

```bash
sudo systemctl edit docker
```

~~The above command generates ```/etc/systemd/system/docker.service.d``` and ```override.conf``` under it~~
Note: this step is not required when installing on Ubuntu 18.04.3 with Kernel 5.0.0

```bash
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
```

```bash
systemctl daemon-reload
systemctl restart docker.service
```

Remember to do this on all other hosts

**Bonus**: if you prefer ```vi``` as default editor on Ubuntu, run ```update-alternatives --config editor```

#### Grant non-root to control Docker
```bash
sudo gpasswd -a $USER docker
```

## Install and Configure Kubernetes

> Reference > https://kubernetes.io/docs/reference/kubectl/cheatsheet/

#### Using China alternative repo for workaround of ```gcr.io``` unaccessible
```bash
sudo apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list \
  deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF

sudo apt-get update
sudo apt install -qy {kubelet,kubectl,kubeadm}=1.15.11-00
```

#### Init cluster by `kubeadm`

Check network pre-requisite before proceeding
> Reference > https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md
and https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

Ensure you pass ```--pod-network-cidr``` param during ```kubeadm init```, which is required by ```flannel``` network

```shell
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 \
  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
  --apiserver-advertise-address 192.168.100.3
  --kubernetes-version "1.15.11"
```

#### Install ```flannel``` network
> Reference > https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

- Pre-requisite
	- For flannel to work correctly, you MUST pass ```--pod-network-cidr=10.244.0.0/16``` to ```kubeadm init```.
	- Set ```/proc/sys/net/bridge/bridge-nf-call-iptables``` to ```1``` by running ```sysctl net.bridge.bridge-nf-call-iptables=1``` to pass bridged IPv4 traffic to iptables’ chains. ...

```bash
kubectl apply -f \
  https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

#### Grant non-root user access to manage K8S
```bash{.line-numbers}
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### ```kubectl``` AutoComplete
```bash{.line-numbers}
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

```bash
# alias and auto-completion
alias k=kubectl
complete -F __start_kubectl k
```



#### Check the status by non-root user
```bash
kubectl get pods -o wide --all-namespaces
```

#### Join K8S cluster

```bash
kubeadm join 10.100.100.11:6443 --token 7li01q.z4d1rcdlowkr7m42 \
  --discovery-token-ca-cert-hash \
  sha256:bbad2c92df60b77d1bb91c5cc56c762b4a736ca942d4c5674eae8b8a634b91f8
```



#### Setup helm

- Install helm and place helm path in $PATH
  - `cp helm /usr/local/bin`
  - `helm init` and `helm init --client-only`


> Reference > https://www.nakivo.com/blog/install-kubernetes-ubuntu/

#### ```helm``` AutoComplete
```bash
source <(helm completion bash)
```



#### Install Tiller

You can search and use alternative image from dockerhub. For example

```bash
docker search tiller
```

Then pick one available from list

```bash
docker pull sapcc/tiller:v2.12.2
docker tag sapcc/tiller:v2.12.2 gcr.io/kubernetes-helm/tiller:v2.12.2
```

> Reference > https://rancher.com/docs/rancher/v2.x/en/installation/ha/helm-init/

```bash
./helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 \
  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
```

(Optional) alternative stable repo update

```bash
helm repo remove stable

helm repo add stable http://mirror.azure.cn/kubernetes/charts/
helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
```




## Operation

#### Update ```dnsConfig``` in ```coredns```

```bash
kubectl -n kube-system edit deployment.apps/coredns
```

Update the correct DNS

> Reference > https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/

```bash
dnsConfig:
  nameservers:
  - 172.16.51.100
  options:
  - name: ndots
    value: "5"
  searches:
  - .com
dnsPolicy: Default
```


#### Connect Private Git Repo by using Personal Token

```bash
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ cp config config.orig
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ git remote rm origin
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ cat config
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[branch "master"]
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ git remote add origin https://j3ffyang:<PERSONAL_TOKEN>@github.com/Vantiq/k8sdeploy_setup.git

ubuntu@vantiq01:~/k8sdeploy_tools/.git$ git remote get-url origin
https://j3ffyang:<PERSONAL_TOKEN>@github.com/Vantiq/k8sdeploy_setup.git

ubuntu@vantiq01:~/k8sdeploy_tools/.git$ cat config
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[branch "master"]
[remote "origin"]
	url = https://j3ffyang:<PERSONAL_TOKEN>@github.com/Vantiq/k8sdeploy_setup.git
	fetch = +refs/heads/*:refs/remotes/origin/*
```

#### Save and Load Image in Docker

- On node where an image resides, for example,

```bash
root@vantiq05:~# docker image list | grep vantiq
vantiq/vantiq-server                                             1.24.12             ab30f4dfd278        6 weeks ago         601MB
vantiq/keycloak                                                  4.2.1.Final         0c41c64e19a4        6 weeks ago         801MB
root@vantiq05:~# docker save -o ./vantiq-server.tar  vantiq/vantiq-server:1.25.6
```

- Copy the tar file to the target node. On node where to load the image,

```bash
root@vantiq06:~# docker load -i ./vantiq-server.tar
```

- After installing the required package, create new docker image

```bash
docker commit -m "<MESSAGE>" -a "<AUTHOR>" [container_name] [image_name]
```

where image_name = ```j3ffyang/ubuntu-nslookup:v1```

- Push new image to ducker hub

```bash
docker push j3ffyang/ubuntu=nslookup:v1
```

#### List all pods by Node

```bash
kubectl get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName \
  --all-namespaces
```

#### SSL Keys in K8S

- List in ```default``` namespace
```bash
kubectl -n default get secrets
```

- Describe the secret, named ```vantiq-cert``` in ```default``` namespace

```bash
kubectl -n default get secret vantiq-cert -o yaml
```

Re-generate secret by using new {cert,key}.pem files

```bash
kubectl -n default create secret tls --cert cert.pem --key key.pem \
  vantiq-cert --dry-run -o yaml | kubectl apply -f -
```

Reference > https://developer.ibm.com/recipes/tutorials/changing-the-tls-certificate-in-ingress-in-information-server-kubernetes-deployments/

In case, you need to create a CSR
```bash
openssl req -new -newkey rsa:2048 -nodes -keyout ingress.key -out ingress.csr
```

Convert p12 to pem
```bash
openssl pkcs7 -print_certs -in certificate.p7b -out ingress.pem  
```

To inspect cert.pem
```bash
openssl x509 -in ingress.pem -text -noout
```

- How to check SSL certificate details in command line
```bash
ubuntu@vantiq2-test02:~$ curl -v https://10.100.102.13:30753/auth -k
*   Trying 10.100.102.13...
* TCP_NODELAY set
* Connected to 10.100.102.13 (10.100.102.13) port 30753 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: C=CN; ST=Beijing; L=Beijing; O=org; OU=IT; CN=eda-dev.org.com; emailAddress=admin@org.com
*  start date: May 31 09:08:11 2019 GMT
*  expire date: May 28 09:08:11 2029 GMT
*  issuer: C=CN; ST=Beijing; L=Beijing; O=org; OU=IT; CN=eda-dev.org.com; emailAddress=admin@org.com
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x564f218d3900)
> GET /auth HTTP/2
> Host: 10.100.102.13:30753
> User-Agent: curl/7.58.0
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
< HTTP/2 303
< server: nginx/1.15.5
< date: Sat, 01 Jun 2019 04:34:41 GMT
< content-length: 0
< location: https://10.100.102.13:30753/auth/
< strict-transport-security: max-age=15724800; includeSubDomains
<
* Connection #0 to host 10.100.102.13 left intact
ubuntu@vantiq2-test02:~$
```



#### Check expiration of SSL Cert

```sh
cd ~/k8sdeploy_tools/targetCluster/deploy/certificates

openssl x509 -in cert.txt -noout -text | grep ' Not '
            Not Before: Nov 11 00:00:00 2019 GMT
            Not After : Nov  7 12:00:00 2020 GMT
```

#### Rotate Kubernetes Certs/ Keys

Kubernetes is using its own signed cert/ key when being installed. You'd need to manually rotate its cert/ key to avoid service interrupted due to cert/ key expiration

> Reference > https://stackoverflow.com/questions/49885636/kubernetes-expired-certificate

```sh
$ cd /etc/kubernetes/pki/
$ mv {apiserver.crt,apiserver-etcd-client.key,apiserver-kubelet-client.crt,front-proxy-ca.crt,front-proxy-client.crt,front-proxy-client.key,front-proxy-ca.key,apiserver-kubelet-client.key,apiserver.key,apiserver-etcd-client.crt} ~/k8s_pki/
$ kubeadm init phase certs all --apiserver-advertise-address <IP>

$ cd /etc/kubernetes/
$ mv {admin.conf,controller-manager.conf,kubelet.conf,scheduler.conf} ~/k8s_pki/
$ kubeadm init phase kubeconfig all
$ reboot

$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
```


#### Kubernetes Backup & Restore

> Reference > https://elastisys.com/2018/12/10/backup-kubernetes-how-and-why/

Backup certificates:

```sh
$ sudo cp -r /etc/kubernetes/pki backup/
```

Make etcd snapshot:

```sh
$ sudo docker run --rm -v $(pwd)/backup:/backup \
	--network host \
	-v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \
	--env ETCDCTL_API=3 \
	k8s.gcr.io/etcd-amd64:3.2.18 \
	etcdctl --endpoints=https://127.0.0.1:2379 \
	--cacert=/etc/kubernetes/pki/etcd/ca.crt \
	--cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
	--key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
	snapshot save /backup/etcd-snapshot-latest.db
```
Backup kubeadm-config:

```sh
$ sudo cp /etc/kubeadm/kubeadm-config.yaml backup/
```

---

Restore certificates

```sh
sudo cp -r backup/pki /etc/kubernetes/
```

Restore `etcd` backup

```sh
sudo mkdir -p /var/lib/etcd
sudo docker run --rm \
    -v $(pwd)/backup:/backup \
    -v /var/lib/etcd:/var/lib/etcd \
    --env ETCDCTL_API=3 \
    k8s.gcr.io/etcd-amd64:3.2.18 \
    /bin/sh -c "etcdctl snapshot restore '/backup/etcd-snapshot-latest.db'; mv /default.etcd/member/ /var/lib/etcd/"
```

Restore `kubeadm-config`

```sh
sudo mkdir /etc/kubeadm
sudo cp backup/kubeadm-config.yaml /etc/kubeadm/
```

Initialize the master with backup

```sh
sudo kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd \
    --config /etc/kubeadm/kubeadm-config.yaml
```		


#### Upgrade Kubernetes to a specific version

```sh
apt install -qy {kubelet,kubectl,kubeadm}=1.15.11-00
```

#### List all running pods on all nodes

To see which pods running on which node

```sh
$ kubectl get pods --all-namespaces -o wide --sort-by="{.spec.nodeName}"

NAMESPACE     NAME                                                    READY   STATUS    RESTARTS   AGE    IP             NODE           
kube-system   kube-proxy-f8xmx                                        1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   etcd-master3-vantiq                                     1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   kube-scheduler-master3-vantiq                           1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   kube-flannel-ds-amd64-blvfv                             1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   coredns-5d4dd4b4db-6m5wq                                1/1     Running   0          82d    10.244.0.2     master3-vantiq
kube-system   coredns-5d4dd4b4db-w9pkv                                1/1     Running   0          82d    10.244.0.3     master3-vantiq
kube-system   kube-apiserver-master3-vantiq                           1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   kube-controller-manager-master3-vantiq                  1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   kube-flannel-ds-amd64-nvhzt                             1/1     Running   1          82d    10.30.10.15    tolerated-node1
shared        telegraf-ds-telegraf-ds-4wwlm                           1/1     Running   0          74d    10.244.11.4    tolerated-node1
default       local-volume-provisioner-v2b7m                          1/1     Running   0          82d    10.244.11.2    tolerated-node1
default       stp-lb-nginx-ingress-default-backend-577dbdff89-dpf5w   1/1     Running   0          73d    10.244.11.5    tolerated-node1
...
```

#### List all pods with `restart`

```sh
$ kubectl get pods --all-namespaces  --sort-by="{.status.containerStatuses[:1].restartCount}"

kube-system   kube-flannel-ds-amd64-9h9qd                             1/1     Running   1          72d
kube-system   kube-flannel-ds-amd64-4gpbv                             1/1     Running   1          83d
default       stp-lb-nginx-ingress-controller-ksdnn                   1/1     Running   1          81d
default       local-volume-provisioner-6p8sb                          1/1     Running   1          83d
kube-system   kube-proxy-47dg6                                        1/1     Running   1          83d
...
```



#### Tear down a cluster
```bash
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
```

```bash
kubeadm reset # on each node
```

> Ref > https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down




#### Audit `mount` and `umount`

This step is an optional but to monitor the unexpected actions of `mount` and `umount` the persistent volumes for MongoDB. This is the configuration of `auditd` in operating system

```sh
cat /etc/audit/rules.d/audit.rules

-a always,exit -F arch=b64 -S mount,umount2 -k mount_umount
```

#### Monitor Mongo disk in Prometheus

This step is optional and to monitor whether there is unexpected `mount` and `umount` of MongoDB's PV. This is the configuration in Prometheus to issue alert when detecting the size of PV changed. `/dev/vdb1` is the device mounted to MongoDB's PV

```sh
node_filesystem_size_bytes{device="/dev/vdb1",name="node10"}
```



#### Force delete a Pod

```sh
kubectl delete pods <pod> --grace-period=0 --force
```


#### MongoDB Backup and Restore with Docker and Kubernetes

> Ref > https://docs.bitnami.com/tutorials/backup-restore-data-mongodb-kubernetes/

- Figure out MongoDB's root passwd

```sh
kubectl -n eda get secret mongodb -o jsonpath="{.data.password}" | base64 --decode
```

- Export MongoDB's service port

```sh
kubectl -n eda get svc

kubectl -n eda port-forward svc/vantiq-eda-mongodb 27017:27017 &
```

- Create mongoDB backup directory

```sh
ubuntu@master3-vantiq:~$ mkdir mongo_backup
ubuntu@master3-vantiq:~$ chmod o+w mongo_backup/
ubuntu@master3-vantiq:~$ cd mongo_backup/
ubuntu@master3-vantiq:~/mongo_backup$
```

- Execute backup

```sh
docker run --rm --name mongodb -v $(pwd):/app --net="host" \
	bitnami/mongodb:latest mongodump -u root -p $PASSWD -o /app
```

- Execute restore

```sh
docker run --rm --name mongodb -v $(pwd):/app --net="host" \
	bitnami/mongodb:latest mongorestore -u root -p $PASSWD /app
```

#### MongoDB Backup (without Docker and Kubernetes)
> Ref > https://www.digitalocean.com/community/tutorials/how-to-back-up-restore-and-migrate-a-mongodb-database-on-ubuntu-14-04

- Create dumpdir

	```sh
	sudo mkdir /var/backups/mongobackups
	sudo mongodump --db newdb --out /var/backups/mongobackups/`date +"%m-%d-%y"`
	```

- Create cronjob

	```sh
	sudo crontab -e
	3 3 * * * mongodump --out /var/backups/mongobackups/`date +"%m-%d-%y"`
	```

- Delete all the backups older than 7 days in order to save space
	```sh
	find /var/backups/mongobackups/ -mtime +7 -exec rm -rf {} \;
	```

- Create delete cronjob

	```sh
	sudo crontab -e
	3 1 * * * find /var/backups/mongobackups/ -mtime +7 -exec rm -rf {} \;
	```

- Restore

	```sh
	sudo mongorestore --db newdb --drop /var/backups/mongobackups/01-20-19/newdb/
	```

#### KeyCloak: update master realm

In case, you need to update `keycloak` master userId if losing its credential.

```sh
kubectl -n shared exec -it keycloak-0 /bin/bash

/opt/jboss/keycloak/bin/add-user-keycloak.sh -r master -u keycloak -p passw0rd
/opt/jboss/keycloak/bin/add-user-keycloak.sh -u keycloak -p passw0rd
/opt/jboss/keycloak/bin/jboss-cli.sh --connect command="reload"
```

If checking Keycloak's PostgreSQL `kubectl -n default logs pod my-pg-release-postgresql-0`

receiving
```sh
2020-03-31 00:46:41.164 GMT [14790] ERROR:  duplicate key value violates unique constraint "uk_ru8tt6t700s9v50bu18ws5ha6"
2020-03-31 00:46:41.164 GMT [14790] DETAIL:  Key (realm_id, username)=(master, keycloak) already exists.
```

You can create another_user then grant it master_realm. Login with "another_user" and update `keycloak` credential



## Troubleshooting

#### When joining the cluster
```bash
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
error execution phase preflight: unable to fetch the kubeadm-config ConfigMap: failed to get config map: Unauthorized
```

It indicates the token expires already by checking
```bash
date
Sun Mar 31 20:17:58 CST 2019

kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
7li01q.z4d1rcdlowkr7m42   <invalid>   2019-03-30T18:49:00+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```

Then create a new one
```bash
kubeadm token create
bgd6cx.2x0fxxqw6cx3l0q4
```

Generate ```sha256 token-ca-cert-hash```
```bash
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

And re-join with the new token

- Label Nodes in Cluster

eg.
```bash
ubuntu@vantiq01:~$ kubectl label node vantiq03 node-role.kubernetes.io/worker=worker
```



#### Exception Received during Deployment ```coredns``` STATUS= ```CrashLoopBackOff```

Cause: Network Configuration on Ubuntu 18.04

Symptom:
```bash
ubuntu@vantiq01:/etc$ kubectl get pods -n=kube-system
NAME                               READY   STATUS             RESTARTS   AGE
coredns-fb8b8dccf-5kz2v            0/1     CrashLoopBackOff   741        2d20h
coredns-fb8b8dccf-8ggcf            0/1     CrashLoopBackOff   741        2d20h
...
```

> Reference > Some explanation from official web
https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/#coredns-pods-have-crashloopbackoff-or-error-state

> The proposed workaround
https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters

> and another good summary and fix
https://stackoverflow.com/questions/53075796/coredns-pods-have-crashloopbackoff-or-error-state/53414041#53414041


- Use ```systemd-resolved.service``` which is by default shipped in Ubuntu 18.04 on each of nodes

- Update ```/run/systemd/resolve/resolv.conf``` with correct DNS on each of nodes

- Restart ```systemd-resolved.service``` on each of nodes
```bash
sudo systemctl restart systemd-resolved.service
```

- Update softlink of ```/etc/resolv.conf``` on each of nodes
```bash
ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
```

- Comment out ```loop``` in

> Reference > https://stackoverflow.com/questions/52645473/coredns-fails-to-run-in-kubernetes-cluster

```
kubectl -n kube-system edit configmap coredns
```

- Delete and restart ```coredns``` pod to take the change effective, then check their status again (warning: make sure you know what you're doing)

```bash
kubectl -n kube-system delete pod -l k8s-app=kube-dns
```


#### Unable to delete an unused PersistentVolume (PV). Alway in ```terminating``` state

After running ```kubectl delete pv local-pv-324352d9```, received

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                                                STORAGECLASS    REASON   AGE
local-pv-324352d9   520Gi      RWO            Retain           Terminating   eda-dev/datadir-vantiq-eda-dev-mongodb-secondary-0   local-storage            2d2h
local-pv-37f6d898   520Gi      RWO            Retain           Terminating   eda-dev/datadir-vantiq-eda-dev-mongodb-primary-0     local-storage            2d2h
```

This is because the PV is protected. Patch it with updating ```finalizers```
```bash
kubectl patch pv local-pv-324352d9 -n ops \
  -p '{"metadata":{"finalizers": []}}' --type=merge
```

To avoid this, you'd have to make sure to umount the mounted disk to such PV before deleting it
