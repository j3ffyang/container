<!--
<img src=../imgs/vantiq_logo2.png width=200px align="right">
<br>
<br>
-->

# Deploy Kubernetes on a Private OpenStack Cloud
<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=4 orderedList=false} -->
<!-- code_chunk_output -->

- [Objective](#objective)
- [Existing Environment](#existing-environment)
- [Architecture Overview and Network Topology](#architecture-overview-and-network-topology)
    - [Environment Stack](#environment-stack)
    - [Network Topology](#network-topology)
- [Security Hardening](#security-hardening)
    - [Operating System on Virtual Machine](#operating-system-on-virtual-machine)
    - [Update Hostname](#update-hostname)
    - [Harden SSHd, then ```systemctl restart sshd.service```](#harden-sshd-then-systemctl-restart-sshdservice)
    - [Turn-on Firewall](#turn-on-firewall)
    - [Modify ```ufw``` on 1st host which has access to the world](#modify-ufw-on-1st-host-which-has-access-to-the-world)
    - [Create a non-root user and grant it ```sudo``` permission](#create-a-non-root-user-and-grant-it-sudo-permission)
- [Setup Network and Disk](#setup-network-and-disk)
    - [Update ```/etc/hosts``` on 1st host](#update-etchosts-on-1st-host)
    - [Setup all hostnames by using ```hostnamectl set-hostname```](#setup-all-hostnames-by-using-hostnamectl-set-hostname)
    - [Setup network to allow other hosts go internet](#setup-network-to-allow-other-hosts-go-internet)
    - [Block Device for Mount](#block-device-for-mount)
- [Install and Configure Docker](#install-and-configure-docker)
    - [Install](#install)
    - [(optional) Use China alternative image repo for performance purpose. Modify ```/etc/docker/daemon.json```](#optional-use-china-alternative-image-repo-for-performance-purpose-modify-etcdockerdaemonjson)
    - [~~Use ```systemd``` to control the Docker daemon~~](#~~use-systemd-to-control-the-docker-daemon~~)
    - [Grant non-root to control Docker](#grant-non-root-to-control-docker)
- [Install and Configure Kubernetes](#install-and-configure-kubernetes)
    - [Using China alternative repo for workaround of ```gcr.io``` unaccessible](#using-china-alternative-repo-for-workaround-of-gcrio-unaccessible)
    - [Init cluster by `kubeadm`](#init-cluster-by-kubeadm)
    - [Install ```flannel``` network](#install-flannel-network)
    - [Grant non-root user access to manage K8S](#grant-non-root-user-access-to-manage-k8s)
    - [```kubectl``` AutoComplete](#kubectl-autocomplete)
    - [Check the status by non-root user](#check-the-status-by-non-root-user)
    - [Join K8S cluster](#join-k8s-cluster)
    - [Setup Helm](#setup-helm)
    - [```helm``` AutoComplete](#helm-autocomplete)
    - [~~Install Tiller~~](#~~install-tiller~~)
- [NFS and Storage](#nfs-and-storage)
    - [Setup an NFS for storageClass](#setup-an-nfs-for-storageclass)
    - [Create storageClass by using NFS](#create-storageclass-by-using-nfs)
    - [Change a storageClass as default](#change-a-storageclass-as-default)
- [Network](#network)
    - [`nginx-ingress`](#nginx-ingress)
    - [Update ```dnsConfig``` in ```coredns```](#update-dnsconfig-in-coredns)
- [Monitoring](#monitoring)
    - [Install Prometheus](#install-prometheus)
    - [Monitor Mongo disk in Prometheus](#monitor-mongo-disk-in-prometheus)
    - [Metrics](#metrics)
    - [Grafana](#grafana)
    - [Prometheus SNMP Exporter](#prometheus-snmp-exporter)
- [SSL and Certificate](#ssl-and-certificate)
    - [`cert-manager`](#cert-manager)
    - [Gen cert and import into Kubernetes cluster](#gen-cert-and-import-into-kubernetes-cluster)
    - [SSL Keys in K8S](#ssl-keys-in-k8s)
    - [Check expiration of SSL Cert](#check-expiration-of-ssl-cert)
    - [Rotate Kubernetes Certs/ Keys](#rotate-kubernetes-certs-keys)
- [Backup and Restore](#backup-and-restore)
    - [Kubernetes Backup & Restore](#kubernetes-backup-restore)
    - [MongoDB Backup and Restore with Docker and Kubernetes](#mongodb-backup-and-restore-with-docker-and-kubernetes)
    - [MongoDB Backup (without Docker and Kubernetes)](#mongodb-backup-without-docker-and-kubernetes)
- [Misc](#misc)
    - [Save and Load Image in Docker](#save-and-load-image-in-docker)
    - [List all pods by Node](#list-all-pods-by-node)
    - [Upgrade Kubernetes to a specific version](#upgrade-kubernetes-to-a-specific-version)
    - [List all running pods on all nodes](#list-all-running-pods-on-all-nodes)
    - [List all pods with `restart`](#list-all-pods-with-restart)
    - [Tear down a cluster](#tear-down-a-cluster)
    - [Force delete a Pod](#force-delete-a-pod)
    - [Audit `mount` and `umount`](#audit-mount-and-umount)
    - [KeyCloak: update master realm](#keycloak-update-master-realm)
    - [Connect Private Git Repo by using Personal Token](#connect-private-git-repo-by-using-personal-token)
- [Troubleshooting](#troubleshooting)
    - [When joining the cluster](#when-joining-the-cluster)
    - [Exception Received during Deployment ```coredns``` STATUS= ```CrashLoopBackOff```](#exception-received-during-deployment-coredns-status-crashloopbackoff)
    - [Unable to delete an unused PersistentVolume (PV). Alway in ```terminating``` state](#unable-to-delete-an-unused-persistentvolume-pv-alway-in-terminating-state)
    - [Output of `cert-manager`](#output-of-cert-manager)
    - [Output of Prometheus](#output-of-prometheus)
    - [Output of `nginx-ingress`](#output-of-nginx-ingress)

<!-- /code_chunk_output -->

## Objective
- Create a standard Kubernetes cluster for production on a standard OpenStack cloud environment without Magnum
- Specify how to create Persistent Volume and Persistent Volume Claim
- Harden the system while connecting to the internet
- Create iptables postrouting rule to enable all nodes access to internet through the 1st machine which has internet access

## Existing Environment

CPU 核 | Memory (G) 内存 | OS Disk (G) | IP | External Disk (G) 外挂磁盘
-- | -- | -- | -- | --
4 | 16 | 40 | 10.100.100.11 |
4 | 16 | 40 | 10.100.100.12 | 530 (MongoDB)
4 | 16 | 40 | 10.100.100.13 | 530 (MongoDB)
4 | 16 | 40 | 10.100.100.14 | 20 (Postgres for KeyCloak)/ 20 (Grafana)/ 20 (GrafanaDB)/ 165 (InfluxDB)
4 | 16 | 40 | 10.100.100.15 | 50 (Nexus)
4 | 16 | 40 | 10.100.100.16 |

> Notice: since the size of disks become a little smaller around 10% after being mounted as persistent volume (PV), so need to give a little more space for this tolerance

## Architecture Overview and Network Topology

#### Environment Stack

<!--
<center><img src="../imgs/20190420_vantiq_k8s_openstack.png"></center>
-->

![k8sarch]

#### Network Topology
We're given total 6 virtual machines and one of 6 has access to internet

<img src="../imgs/20190327_cp_nw_top.png">

## Security Hardening

#### Operating System on Virtual Machine

```bash
root@vantiq01:~# cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION="Ubuntu 18.04.1 LTS"
```

#### Update Hostname
```bash
hostnamectl set-hostname vantiq01
```

#### Harden SSHd, then ```systemctl restart sshd.service```
```bash
PermitRootLogin prohibit-password

PubkeyAuthentication yes
PasswordAuthentication no
```

#### Turn-on Firewall
```sh
ufw allow ssh
ufw enable
```

#### Modify ```ufw``` on 1st host which has access to the world
```bash
sudo ufw status numbered
Status: active

     To                         Action      From
     --                         ------      ----
[ 1] OpenSSH                    ALLOW IN    Anywhere
[ 2] 22/tcp                     ALLOW IN    Anywhere
[ 3] 6443                       ALLOW IN    Anywhere
```

Generally, the iptables rule should allow the following access requirements

1. Request from ```10.100.100.0/24``` to ```10.100.100.11:6443```
2. Request from ```10.224.0.0/24``` to  ```10.100.100.11:6443``` by ```flannel```
3. The incoming from random Github IPs to ```10.100.100.11:6443```, associated to outgoing traffic when image download is initialized

Relative port required by Kubernetes > https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports


#### Create a non-root user and grant it ```sudo``` permission
```bash
adduser ubuntu
usermod -aG sudo ubuntu
```


## Setup Network and Disk

#### Update ```/etc/hosts``` on 1st host
```bash
10.100.100.11	vantiq01	vantiq01
10.100.100.12	vantiq02	vantiq02
10.100.100.13	vantiq03	vantiq03
10.100.100.14	vantiq04	vantiq04
10.100.100.15	vantiq05	vantiq05
10.100.100.16	vantiq06	vantiq06
```

#### Setup all hostnames by using ```hostnamectl set-hostname```

In Bash shell, run
```bash
for i in {2..6}; do ssh root@vantiq0$i \
  -i ~/.ssh/Vantiq-key.pem "hostnamectl set-hostname vantiq0$i"; done
```

```bash
for i in {2..6}; do ssh root@vantiq0$i -i ~/.ssh/Vantiq-key.pem "hostname"; done
```

#### Setup network to allow other hosts go internet
Since the 1st host is the only one allowed to access internet and others are off the world, here are the steps to route network requests from other hosts through the 1st one to internet

On the 1st host as ```gateway```, use ```POSTROUTING``` rule to ```MASQUERADE``` all traffic from ```10.100.100.0/24``` pass through ```10.100.100.11``` to reach outside. If ```ufw``` is on, need to define a rule for incoming traffic from ```10.100.100.0/24```

```bash
iptables -t nat -A POSTROUTING -s 10.100.100.0/24 -j MASQUERADE
ufw allow in on eth0 from 10.100.100.0/24
```

Update 1st host's ```/etc/default/ufw``` configuration to allow forward, then ```systemctl restart ufw.service```
```bash
DEFAULT_FORWARD_POLICY="ACCEPT"
```

On other hosts
```bash
ip r add default via 10.100.100.11 dev eth0
```

> Note: you might have to delete the pre-configured default gateway. Caution: be careful when you do this and you know what exactly you're doing!

#### Block Device for Mount

```bash
for i in {2..4}; do ssh root@vantiq0$i -i ~/.ssh/Vantiq-key.pem 'hostname; lsblk'; done
vantiq02
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0  530G  0 disk
└─vdb1 252:17   0  530G  0 part
vantiq03
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0  530G  0 disk
vantiq04
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   40G  0 disk
└─vda1 252:1    0   40G  0 part /
vdb    252:16   0   20G  0 disk
vdc    252:32   0  160G  0 disk
vdg    252:96   0   20G  0 disk
```

## Install and Configure Docker

#### Install
> Reference > https://docs.docker.com/install/linux/docker-ce/ubuntu/

```bash {.line-numbers}
sudo apt install apt-transport-https ca-certificates curl software-properties-common

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

sudo add-apt-repository \
  "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

sudo apt update
sudo apt install docker-ce
```

#### (optional) Use China alternative image repo for performance purpose. Modify ```/etc/docker/daemon.json```
> Reference > https://www.docker-cn.com/registry-mirror

```json
{
  "registry-mirrors": ["https://registry.docker-cn.com"]
}
```

#### ~~Use ```systemd``` to control the Docker daemon~~
Overriding Defaults for the Docker Daemon

```bash
sudo systemctl edit docker
```

~~The above command generates ```/etc/systemd/system/docker.service.d``` and ```override.conf``` under it~~
Note: this step is not required when installing on Ubuntu 18.04.3 with Kernel 5.0.0

```bash
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
```

```bash
systemctl daemon-reload
systemctl restart docker.service
```

Remember to do this on all other hosts

**Bonus**: if you prefer ```vi``` as default editor on Ubuntu, run ```update-alternatives --config editor```

#### Grant non-root to control Docker
```bash
sudo gpasswd -a $USER docker
```

## Install and Configure Kubernetes

> Reference > https://kubernetes.io/docs/reference/kubectl/cheatsheet/

#### Using China alternative repo for workaround of ```gcr.io``` unaccessible
```bash
sudo apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list \
  deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF

sudo apt-get update
sudo apt install -qy {kubelet,kubectl,kubeadm}=1.15.11-00
```

#### Init cluster by `kubeadm`

Check network pre-requisite before proceeding
> Reference > https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md
and https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

Ensure you pass ```--pod-network-cidr``` param during ```kubeadm init```, which is required by ```flannel``` network

```shell
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 \
  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
  --apiserver-advertise-address 192.168.100.3
  --kubernetes-version "1.15.11"
```

#### Install ```flannel``` network
> Reference > https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

- Pre-requisite
	- For flannel to work correctly, you MUST pass ```--pod-network-cidr=10.244.0.0/16``` to ```kubeadm init```.
	- Set ```/proc/sys/net/bridge/bridge-nf-call-iptables``` to ```1``` by running ```sysctl net.bridge.bridge-nf-call-iptables=1``` to pass bridged IPv4 traffic to iptables’ chains. ...

```bash
kubectl apply -f \
  https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

#### Grant non-root user access to manage K8S
```bash{.line-numbers}
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### ```kubectl``` AutoComplete
```bash{.line-numbers}
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

```bash
# alias and auto-completion
alias k=kubectl
complete -F __start_kubectl k
```



#### Check the status by non-root user
```bash
kubectl get pods -o wide --all-namespaces
```

#### Join K8S cluster

```bash
kubeadm join 10.100.100.11:6443 --token 7li01q.z4d1rcdlowkr7m42 \
  --discovery-token-ca-cert-hash \
  sha256:bbad2c92df60b77d1bb91c5cc56c762b4a736ca942d4c5674eae8b8a634b91f8
```



#### Setup Helm

Install helm and place helm path in $PATH
- `cp helm /usr/local/bin`
- `helm init` and `helm init --client-only`

> Reference > https://www.nakivo.com/blog/install-kubernetes-ubuntu/


(Optional) alternative stable repo update

```bash
helm repo remove stable

helm repo add stable http://mirror.azure.cn/kubernetes/charts/
helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
```

#### ```helm``` AutoComplete
```bash
source <(helm completion bash)
```

> Reference >
https://github.com/helm/charts
https://github.com/cloudnativeapp/workshop/tree/master/kubecon2019china/charts/guestbook


#### ~~Install Tiller~~

You can search and use alternative image from dockerhub. For example

```bash
docker search tiller
```

Then pick one available from list

```bash
docker pull sapcc/tiller:v2.12.2
docker tag sapcc/tiller:v2.12.2 gcr.io/kubernetes-helm/tiller:v2.12.2
```

> Reference > https://rancher.com/docs/rancher/v2.x/en/installation/ha/helm-init/

```bash
./helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 \
  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
```




## NFS and Storage

#### Setup an NFS for storageClass

- On server
```sh
sudo apt install nfs-kernel-server -y

sudo mkdir -p /mnt/nfs
echo `/mnt/nfs   *(rw,sync,no_root_squash)` >> /etc/exports
```

Find device UUID

```sh
ubuntu@smoke-disk01:~$ sudo blkid
[sudo] password for ubuntu:
/dev/vda1: UUID="ebf7b11c-e1bc-4725-a0d2-d42aee76509f" TYPE="ext4" PARTUUID="e6bc4008-01"
/dev/vdb1: UUID="b9ebee1c-1db6-4fa0-8adb-892337e0a8f5" TYPE="ext4" PARTUUID="89de0170-01"
```

Place the following into `/etc/fstab`
```sh
UUID=b9ebee1c-1db6-4fa0-8adb-892337e0a8f5 /mnt/nfs    ext4    rw,user,noauto,exec,utf8    0    1
```

Mount
```sh
sudo mount -a
```

- On ALL clients/ nodes in Kubernetes cluster

```sh
sudo apt-get install nfs-common -y
```

Place the following into `/etc/fstab`
```sh
172.16.50.249:/mnt/nfs	/mnt/nfs	nfs	defaults 0 0
```

where `172.16.50.249` is NFS server


#### Create storageClass by using NFS

```sh
helm install nfs \
  --set nfs.server=172.16.50.249 \
  --set nfs.path=/mnt/nfs \
  stable/nfs-client-provisioner
```


#### Change a storageClass as default

Reference
> https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/

- Change default tag

```sh
ubuntu@vantiq2-test01:~$ kubectl -n default get sc
NAME                      PROVISIONER                                           AGE
local-storage (default)   kubernetes.io/no-provisioner                          374d
nfs-client                cluster.local/my-nfs-release-nfs-client-provisioner   3m57s

ubuntu@vantiq2-test01:~$ kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
storageclass.storage.k8s.io/local-storage patched

ubuntu@vantiq2-test01:~$ kubectl -n default get sc
NAME            PROVISIONER                                           AGE
local-storage   kubernetes.io/no-provisioner                          374d
nfs-client      cluster.local/my-nfs-release-nfs-client-provisioner   10m

ubuntu@vantiq2-test01:~$ kubectl patch storageclass nfs-client -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
storageclass.storage.k8s.io/nfs-client patched

ubuntu@vantiq2-test01:~$ kubectl -n default get sc
NAME                   PROVISIONER                                           AGE
local-storage          kubernetes.io/no-provisioner                          374d
nfs-client (default)   cluster.local/my-nfs-release-nfs-client-provisioner   10m
ubuntu@vantiq2-test01:~$
```


## Network

#### `nginx-ingress`

> Reference >
https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/
https://github.com/nginxinc/kubernetes-ingress


```sh
helm repo add nginx-stable https://helm.nginx.com/stable
helm repo update

helm install nginx-ingress nginx-stable/nginx-ingress \
  --namespace nginx-ingress \
  --set controller.metrics.enabled=true \
  --set controller.service.type=NodePort \
  --set controller.service.nodePorts.http=30080 \
  --set controller.service.nodePorts.https=30443
```


#### Update ```dnsConfig``` in ```coredns```

```bash
kubectl -n kube-system edit deployment.apps/coredns
```

Update the correct DNS

> Reference >
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/

```bash
dnsConfig:
  nameservers:
  - 172.16.51.100
  options:
  - name: ndots
    value: "5"
  searches:
  - .com
dnsPolicy: Default
```


## Monitoring

#### Install Prometheus

```sh
helm install prometheus stable/prometheus --namespace=monitoring
```

#### Monitor Mongo disk in Prometheus

This step is optional and to monitor whether there is unexpected `mount` and `umount` of MongoDB's PV. This is the configuration in Prometheus to issue alert when detecting the size of PV changed. `/dev/vdb1` is the device mounted to MongoDB's PV

```sh
node_filesystem_size_bytes{device="/dev/vdb1",name="node10"}
```

#### Metrics

```sh
helm repo add bitnami https://charts.bitnami.com/bitnami

helm install metrics bitnami/metrics-server --namespace monitoring

helm upgrade metrics bitnami/metrics-server --namespace monitoring \
  --set apiService.create=true
```

Edit deployment and add the following args
```sh
k -n monitoring edit deployments.apps metrics-metrics-server
```

```sh
    spec:
      containers:
      - args:
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP
```

Check whether CPU metrics available

```sh
kubectl get --raw /api/v1/nodes/{NODE_NAME}/proxy/stats/summary | jq '.node'
```

#### Grafana

```sh
ubuntu@smoke01:~/cert-mgr$ helm install grafana stable/grafana --namespace monitoring --set persistence.enabled=true
NAME: grafana
LAST DEPLOYED: Mon Sep  7 22:27:22 2020
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
1. Get your 'admin' user password by running:

   kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.monitoring.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:

     export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace monitoring port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin
```

#### Prometheus SNMP Exporter
> Reference >
> https://github.com/prometheus-community/helm-charts
> https://github.com/helm/charts/tree/master/stable/prometheus-snmp-exporter

```sh
helm install snmp-exporter stable/prometheus-snmp-exporter -n monitoring
```

Generate >
https://github.com/prometheus/snmp_exporter/tree/master/generator

```sh
# Debian-based distributions.
sudo apt-get install unzip build-essential libsnmp-dev p7zip-full # Debian-based distros
# Redhat-based distributions.
sudo yum install gcc gcc-g++ make net-snmp net-snmp-utils net-snmp-libs net-snmp-devel # RHEL-based distros

go get github.com/prometheus/snmp_exporter/generator
cd ${GOPATH-$HOME/go}/src/github.com/prometheus/snmp_exporter/generator
go build
make mibs
```

https://grafana.com/grafana/dashboards/1124


## SSL and Certificate

#### `cert-manager`

> Reference >
https://hub.helm.sh/charts/jetstack/cert-manager
https://cert-manager.io/docs/configuration/selfsigned/

According to https://cert-manager.io/docs/installation/kubernetes/

```sh
ubuntu@smoke01:~$ helm repo add jetstack https://charts.jetstack.io

ubuntu@smoke01:~$ k create ns cert-manager
namespace/cert-manager created

helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version v0.16.1 \
  --set installCRDs=true
```

#### Gen cert and import into Kubernetes cluster

- Gen
```sh
openssl req -x509 -new -nodes -key ca.key -subj "/CN=example" -days 3650 -reqexts v3_req -extensions v3_ca -out ca.crt
Can't load /home/ubuntu/.rnd into RNG
139800436474304:error:2406F079:random number generator:RAND_load_file:Cannot open file:../crypto/rand/randfile.c:88:Filename=/home/ubuntu/.rnd
```

```sh
openssl req -x509 -new -nodes -key ca.key -subj "/CN=example" -days 3650 -out ca.crt
```

```sh
cd ~/; openssl rand -writerand .rnd
```

or edit `/etc/ssl/openssl.cnf` and remove
```sh
RANDFILE = $ENV::EASYRSA_PKI/.rnd
```

- Import `ca.{crt,key}` into Kubernetes

```sh
k create secret tls ca-key-pair --cert=ca.crt --key=ca.key --namespace=cert-manager
```

> Must be in `cert-manager` namespace if using `ClusterIssuer`

- Create `ClusterIssuer` resource

```sh
ubuntu@smoke01:~/cert-mgr$ cat clusterissuer.yaml
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: clusterissuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair
```

```sh
k apply -f clusterissuer.yaml
```

- Create `Certificate` resource

```sh
ubuntu@smoke01:~/cert-mgr$ cat prom_cert.yaml

apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: example-com
  namespace: monitoring
spec:
  secretName: example-com-tls
  issuerRef:
    name: clusterissuer
    kind: ClusterIssuer
  commonName: example.com
  organization:
  - CA
  dnsNames:
  - example.com
```

- Create `Ingress`

```sh
ubuntu@smoke01:~/cert-mgr$ cat prom_ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    cert-manager.io/cluster-issuer: clusterissuer
  name: ingress-prom
  namespace: monitoring
spec:
  rules:
  - host: prometheus.example.com
    http:
      paths:
      - backend:
          serviceName: prometheus-server
          servicePort: 80
        path: /
  tls:
  - hosts:
    - prometheus.example.com
    secretName: example-com-tls
```





#### SSL Keys in K8S

- List in ```default``` namespace
```bash
kubectl -n default get secrets
```

- Describe the secret, named ```vantiq-cert``` in ```default``` namespace

```bash
kubectl -n default get secret vantiq-cert -o yaml
```

Re-generate secret by using new {cert,key}.pem files

```bash
kubectl -n default create secret tls --cert cert.pem --key key.pem \
  vantiq-cert --dry-run -o yaml | kubectl apply -f -
```

Reference > https://developer.ibm.com/recipes/tutorials/changing-the-tls-certificate-in-ingress-in-information-server-kubernetes-deployments/

In case, you need to create a CSR
```bash
openssl req -new -newkey rsa:2048 -nodes -keyout ingress.key -out ingress.csr
```

Convert p12 to pem
```bash
openssl pkcs7 -print_certs -in certificate.p7b -out ingress.pem  
```

To inspect cert.pem
```bash
openssl x509 -in ingress.pem -text -noout
```

- How to check SSL certificate details in command line
```bash
ubuntu@vantiq2-test02:~$ curl -v https://10.100.102.13:30753/auth -k
*   Trying 10.100.102.13...
* TCP_NODELAY set
* Connected to 10.100.102.13 (10.100.102.13) port 30753 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: C=CN; ST=Beijing; L=Beijing; O=org; OU=IT; CN=eda-dev.org.com; emailAddress=admin@org.com
*  start date: May 31 09:08:11 2019 GMT
*  expire date: May 28 09:08:11 2029 GMT
*  issuer: C=CN; ST=Beijing; L=Beijing; O=org; OU=IT; CN=eda-dev.org.com; emailAddress=admin@org.com
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x564f218d3900)
> GET /auth HTTP/2
> Host: 10.100.102.13:30753
> User-Agent: curl/7.58.0
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
< HTTP/2 303
< server: nginx/1.15.5
< date: Sat, 01 Jun 2019 04:34:41 GMT
< content-length: 0
< location: https://10.100.102.13:30753/auth/
< strict-transport-security: max-age=15724800; includeSubDomains
<
* Connection #0 to host 10.100.102.13 left intact
ubuntu@vantiq2-test02:~$
```



#### Check expiration of SSL Cert

```sh
cd ~/k8sdeploy_tools/targetCluster/deploy/certificates

openssl x509 -in cert.txt -noout -text | grep ' Not '
            Not Before: Nov 11 00:00:00 2019 GMT
            Not After : Nov  7 12:00:00 2020 GMT
```

#### Rotate Kubernetes Certs/ Keys

Kubernetes is using its own signed cert/ key when being installed. You'd need to manually rotate its cert/ key to avoid service interrupted due to cert/ key expiration

> Reference > https://stackoverflow.com/questions/49885636/kubernetes-expired-certificate

```sh
$ cd /etc/kubernetes/pki/
$ mv {apiserver.crt,apiserver-etcd-client.key,apiserver-kubelet-client.crt,front-proxy-ca.crt,front-proxy-client.crt,front-proxy-client.key,front-proxy-ca.key,apiserver-kubelet-client.key,apiserver.key,apiserver-etcd-client.crt} ~/k8s_pki/
$ kubeadm init phase certs all --apiserver-advertise-address <IP>

$ cd /etc/kubernetes/
$ mv {admin.conf,controller-manager.conf,kubelet.conf,scheduler.conf} ~/k8s_pki/
$ kubeadm init phase kubeconfig all
$ reboot

$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
```


## Backup and Restore


#### Kubernetes Backup & Restore

> Reference > https://elastisys.com/2018/12/10/backup-kubernetes-how-and-why/

Backup certificates:

```sh
$ sudo cp -r /etc/kubernetes/pki backup/
```

Make etcd snapshot:

```sh
$ sudo docker run --rm -v $(pwd)/backup:/backup \
	--network host \
	-v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \
	--env ETCDCTL_API=3 \
	k8s.gcr.io/etcd-amd64:3.2.18 \
	etcdctl --endpoints=https://127.0.0.1:2379 \
	--cacert=/etc/kubernetes/pki/etcd/ca.crt \
	--cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
	--key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
	snapshot save /backup/etcd-snapshot-latest.db
```
Backup kubeadm-config:

```sh
$ sudo cp /etc/kubeadm/kubeadm-config.yaml backup/
```

---

Restore certificates

```sh
sudo cp -r backup/pki /etc/kubernetes/
```

Restore `etcd` backup

```sh
sudo mkdir -p /var/lib/etcd
sudo docker run --rm \
    -v $(pwd)/backup:/backup \
    -v /var/lib/etcd:/var/lib/etcd \
    --env ETCDCTL_API=3 \
    k8s.gcr.io/etcd-amd64:3.2.18 \
    /bin/sh -c "etcdctl snapshot restore '/backup/etcd-snapshot-latest.db'; mv /default.etcd/member/ /var/lib/etcd/"
```

Restore `kubeadm-config`

```sh
sudo mkdir /etc/kubeadm
sudo cp backup/kubeadm-config.yaml /etc/kubeadm/
```

Initialize the master with backup

```sh
sudo kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd \
    --config /etc/kubeadm/kubeadm-config.yaml
```		


#### MongoDB Backup and Restore with Docker and Kubernetes

> Ref > https://docs.bitnami.com/tutorials/backup-restore-data-mongodb-kubernetes/

- Figure out MongoDB's root passwd

```sh
kubectl -n eda get secret mongodb -o jsonpath="{.data.password}" | base64 --decode
```

- Export MongoDB's service port

```sh
kubectl -n eda get svc

kubectl -n eda port-forward svc/vantiq-eda-mongodb 27017:27017 &
```

- Create mongoDB backup directory

```sh
ubuntu@master3-vantiq:~$ mkdir mongo_backup
ubuntu@master3-vantiq:~$ chmod o+w mongo_backup/
ubuntu@master3-vantiq:~$ cd mongo_backup/
ubuntu@master3-vantiq:~/mongo_backup$
```

- Execute backup

```sh
docker run --rm --name mongodb -v $(pwd):/app --net="host" \
	bitnami/mongodb:latest mongodump -u root -p $PASSWD -o /app
```

- Execute restore

```sh
docker run --rm --name mongodb -v $(pwd):/app --net="host" \
	bitnami/mongodb:latest mongorestore -u root -p $PASSWD /app
```

#### MongoDB Backup (without Docker and Kubernetes)
> Ref > https://www.digitalocean.com/community/tutorials/how-to-back-up-restore-and-migrate-a-mongodb-database-on-ubuntu-14-04

- Create dumpdir

	```sh
	sudo mkdir /var/backups/mongobackups
	sudo mongodump --db newdb --out /var/backups/mongobackups/`date +"%m-%d-%y"`
	```

- Create cronjob

	```sh
	sudo crontab -e
	3 3 * * * mongodump --out /var/backups/mongobackups/`date +"%m-%d-%y"`
	```

- Delete all the backups older than 7 days in order to save space
	```sh
	find /var/backups/mongobackups/ -mtime +7 -exec rm -rf {} \;
	```

- Create delete cronjob

	```sh
	sudo crontab -e
	3 1 * * * find /var/backups/mongobackups/ -mtime +7 -exec rm -rf {} \;
	```

- Restore

	```sh
	sudo mongorestore --db newdb --drop /var/backups/mongobackups/01-20-19/newdb/
	```

## Misc

#### Save and Load Image in Docker

- On node where an image resides, for example,

```bash
root@vantiq05:~# docker image list | grep vantiq
vantiq/vantiq-server                                             1.24.12             ab30f4dfd278        6 weeks ago         601MB
vantiq/keycloak                                                  4.2.1.Final         0c41c64e19a4        6 weeks ago         801MB
root@vantiq05:~# docker save -o ./vantiq-server.tar  vantiq/vantiq-server:1.25.6
```

- Copy the tar file to the target node. On node where to load the image,

```bash
root@vantiq06:~# docker load -i ./vantiq-server.tar
```

- After installing the required package, create new docker image

```bash
docker commit -m "<MESSAGE>" -a "<AUTHOR>" [container_name] [image_name]
```

where image_name = ```j3ffyang/ubuntu-nslookup:v1```

- Push new image to ducker hub

```bash
docker push j3ffyang/ubuntu=nslookup:v1
```

#### List all pods by Node

```bash
kubectl get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName \
  --all-namespaces
```




#### Upgrade Kubernetes to a specific version

```sh
apt install -qy {kubelet,kubectl,kubeadm}=1.15.11-00
```

#### List all running pods on all nodes

To see which pods running on which node

```sh
$ kubectl get pods --all-namespaces -o wide --sort-by="{.spec.nodeName}"

NAMESPACE     NAME                                                    READY   STATUS    RESTARTS   AGE    IP             NODE           
kube-system   kube-proxy-f8xmx                                        1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   etcd-master3-vantiq                                     1/1     Running   0          82d    10.30.10.13    master3-vantiq
kube-system   kube-scheduler-master3-vantiq                           1/1     Running   0          82d    10.30.10.13    master3-vantiq
...
```

#### List all pods with `restart`

```sh
$ kubectl get pods --all-namespaces  --sort-by="{.status.containerStatuses[:1].restartCount}"

kube-system   kube-flannel-ds-amd64-9h9qd                             1/1     Running   1          72d
kube-system   kube-flannel-ds-amd64-4gpbv                             1/1     Running   1          83d
kube-system   kube-proxy-47dg6                                        1/1     Running   1          83d
...
```



#### Tear down a cluster
```bash
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>

kubeadm reset # on each node
```

> Ref > https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down

#### Force delete a Pod

```sh
kubectl delete pods <pod> --grace-period=0 --force
```


#### Audit `mount` and `umount`

This step is an optional but to monitor the unexpected actions of `mount` and `umount` the persistent volumes for MongoDB. This is the configuration of `auditd` in operating system

```sh
cat /etc/audit/rules.d/audit.rules

-a always,exit -F arch=b64 -S mount,umount2 -k mount_umount
```



#### KeyCloak: update master realm

In case, you need to update `keycloak` master userId if losing its credential.

```sh
kubectl -n shared exec -it keycloak-0 /bin/bash

/opt/jboss/keycloak/bin/add-user-keycloak.sh -r master -u keycloak -p passw0rd
/opt/jboss/keycloak/bin/add-user-keycloak.sh -u keycloak -p passw0rd
/opt/jboss/keycloak/bin/jboss-cli.sh --connect command="reload"
```

If checking Keycloak's PostgreSQL `kubectl -n default logs pod my-pg-release-postgresql-0`

receiving
```sh
2020-03-31 00:46:41.164 GMT [14790] ERROR:  duplicate key value violates unique constraint "uk_ru8tt6t700s9v50bu18ws5ha6"
2020-03-31 00:46:41.164 GMT [14790] DETAIL:  Key (realm_id, username)=(master, keycloak) already exists.
```

You can create another_user then grant it master_realm. Login with "another_user" and update `keycloak` credential

#### Connect Private Git Repo by using Personal Token

```bash
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ cp config config.orig
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ git remote rm origin
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ cat config
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[branch "master"]
ubuntu@vantiq01:~/k8sdeploy_tools/.git$ git remote add origin https://j3ffyang:<PERSONAL_TOKEN>@github.com/Vantiq/k8sdeploy_setup.git

ubuntu@vantiq01:~/k8sdeploy_tools/.git$ git remote get-url origin
https://j3ffyang:<PERSONAL_TOKEN>@github.com/Vantiq/k8sdeploy_setup.git

ubuntu@vantiq01:~/k8sdeploy_tools/.git$ cat config
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[branch "master"]
[remote "origin"]
	url = https://j3ffyang:<PERSONAL_TOKEN>@github.com/Vantiq/k8sdeploy_setup.git
	fetch = +refs/heads/*:refs/remotes/origin/*
```

## Troubleshooting

#### When joining the cluster
```bash
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
error execution phase preflight: unable to fetch the kubeadm-config ConfigMap: failed to get config map: Unauthorized
```

It indicates the token expires already by checking
```bash
date
Sun Mar 31 20:17:58 CST 2019

kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
7li01q.z4d1rcdlowkr7m42   <invalid>   2019-03-30T18:49:00+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```

Then create a new one
```bash
kubeadm token create
bgd6cx.2x0fxxqw6cx3l0q4
```

Generate ```sha256 token-ca-cert-hash```
```bash
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

And re-join with the new token

- Label Nodes in Cluster

eg.
```bash
ubuntu@vantiq01:~$ kubectl label node vantiq03 node-role.kubernetes.io/worker=worker
```



#### Exception Received during Deployment ```coredns``` STATUS= ```CrashLoopBackOff```

Cause: Network Configuration on Ubuntu 18.04

Symptom:
```bash
ubuntu@vantiq01:/etc$ kubectl get pods -n=kube-system
NAME                               READY   STATUS             RESTARTS   AGE
coredns-fb8b8dccf-5kz2v            0/1     CrashLoopBackOff   741        2d20h
coredns-fb8b8dccf-8ggcf            0/1     CrashLoopBackOff   741        2d20h
...
```

> Reference > Some explanation from official web
https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/#coredns-pods-have-crashloopbackoff-or-error-state

> The proposed workaround
https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters

> and another good summary and fix
https://stackoverflow.com/questions/53075796/coredns-pods-have-crashloopbackoff-or-error-state/53414041#53414041


- Use ```systemd-resolved.service``` which is by default shipped in Ubuntu 18.04 on each of nodes

- Update ```/run/systemd/resolve/resolv.conf``` with correct DNS on each of nodes

- Restart ```systemd-resolved.service``` on each of nodes
```bash
sudo systemctl restart systemd-resolved.service
```

- Update softlink of ```/etc/resolv.conf``` on each of nodes
```bash
ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
```

- Comment out ```loop``` in

> Reference > https://stackoverflow.com/questions/52645473/coredns-fails-to-run-in-kubernetes-cluster

```
kubectl -n kube-system edit configmap coredns
```

- Delete and restart ```coredns``` pod to take the change effective, then check their status again (warning: make sure you know what you're doing)

```bash
kubectl -n kube-system delete pod -l k8s-app=kube-dns
```


#### Unable to delete an unused PersistentVolume (PV). Alway in ```terminating``` state

After running ```kubectl delete pv local-pv-324352d9```, received

```bash
ubuntu@vantiq2-test01:~/k8sdeploy_tools$ kubectl get pv
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                                                STORAGECLASS    REASON   AGE
local-pv-324352d9   520Gi      RWO            Retain           Terminating   eda-dev/datadir-vantiq-eda-dev-mongodb-secondary-0   local-storage            2d2h
local-pv-37f6d898   520Gi      RWO            Retain           Terminating   eda-dev/datadir-vantiq-eda-dev-mongodb-primary-0     local-storage            2d2h
```

This is because the PV is protected. Patch it with updating ```finalizers```
```bash
kubectl patch pv local-pv-324352d9 -n ops \
  -p '{"metadata":{"finalizers": []}}' --type=merge
```

To avoid this, you'd have to make sure to umount the mounted disk to such PV before deleting it

#### Output of `cert-manager`

```sh
ubuntu@smoke01:~$ helm install cert-manager jetstack/cert-manager --namespace cert-manager
NAME: cert-manager
LAST DEPLOYED: Mon Aug 31 10:26:29 2020
NAMESPACE: cert-manager
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
cert-manager has been deployed successfully!

In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).

More information on the different types of issuers and how to configure them
can be found in our documentation:

https://cert-manager.io/docs/configuration/

For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:

https://cert-manager.io/docs/usage/ingress/
```

#### Output of Prometheus

```sh
ubuntu@smoke01:~$ helm install prometheus stable/prometheus --namespace=monitoring
NAME: prometheus
LAST DEPLOYED: Sun Aug 30 12:50:07 2020
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus,component=alertmanager" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been moved to a global property.  #####
######            use .Values.podSecurityPolicy.enabled with pod-based      #####
######            annotations                                               #####
######            (e.g. .Values.nodeExporter.podSecurityPolicy.annotations) #####
#################################################################################


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-pushgateway.monitoring.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/
```

#### Output of `nginx-ingress`

```sh
ubuntu@smoke01:~$ helm install nginx-ingress stable/nginx-ingress --namespace nginx-ingress --set controller.metrics.enabled=true --set controller.service.type=NodePort --set controller.service.nodePorts.http=30080 --set controller.service.nodePorts.https=30443
WARNING: This chart is deprecated
NAME: nginx-ingress
LAST DEPLOYED: Sun Aug 30 13:15:31 2020
NAMESPACE: nginx-ingress
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
*******************************************************************************************************
* DEPRECATED, please use https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx *
*******************************************************************************************************


The nginx-ingress controller has been installed.
Get the application URL by running these commands:
  export HTTP_NODE_PORT=30080
  export HTTPS_NODE_PORT=30443
  export NODE_IP=$(kubectl --namespace nginx-ingress get nodes -o jsonpath="{.items[0].status.addresses[1].address}")

  echo "Visit http://$NODE_IP:$HTTP_NODE_PORT to access your application via HTTP."
  echo "Visit https://$NODE_IP:$HTTPS_NODE_PORT to access your application via HTTPS."

An example Ingress that makes use of the controller:

  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
    name: example
    namespace: foo
  spec:
    rules:
      - host: www.example.com
        http:
          paths:
            - backend:
                serviceName: exampleService
                servicePort: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
        - hosts:
            - www.example.com
          secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls
```

---


[k8sarch]:data:image/png;base64,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
